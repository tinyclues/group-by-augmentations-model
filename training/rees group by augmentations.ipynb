{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea932e5b",
   "metadata": {},
   "source": [
    "# Bi-linear interaction model with group-by augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6f05f",
   "metadata": {},
   "source": [
    "## Prepare dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96897ae",
   "metadata": {},
   "source": [
    "Following the same steps as in [the training of a simple model](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/training/movielens%20simple%20model.ipynb), we first load splitted dataset generated in [notebook](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/dataset_preprocessing/movielens%20with%20imdb.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1899e738-56d3-455f-afcd-ce80fcb17232",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'rees_ecommerce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19cc05cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 22:04:22.130831: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-05 22:04:22.315242: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2023-03-05 22:04:22.315283: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-05 22:04:23.539241: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2023-03-05 22:04:23.539518: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2023-03-05 22:04:23.539544: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/recsys-multi-atrribute-benchmark/training/utils.py:24: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.load(...)` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 22:04:25.410985: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2023-03-05 22:04:25.411043: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-05 22:04:25.411088: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (d0c4fb6a256c): /proc/driver/nvidia/version does not exist\n",
      "2023-03-05 22:04:25.411358: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from utils import load_dataset\n",
    "\n",
    "datasets = {}\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    datasets[split_name] = load_dataset(DATASET, split_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d1b29e",
   "metadata": {},
   "source": [
    "Then we parse features' names to obtain a list of offer features (that will be used to modelize film) and a list of user features (aggregated history up to chosen date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5077ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import AGG_PREFIX\n",
    "\n",
    "all_columns = list(datasets['train'].element_spec.keys())\n",
    "technical_columns = ['user_id', 'date']\n",
    "user_features = list(filter(lambda x: x.startswith(AGG_PREFIX), all_columns))\n",
    "offer_features = list(filter(lambda x: x not in user_features + technical_columns, all_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d5781",
   "metadata": {},
   "source": [
    "### Rebatching datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96990cb",
   "metadata": {},
   "source": [
    "Splitting dataset into smaller batches in the same way as described in [the training of a simple model](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/training/movielens%20simple%20model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa53a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.7/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "CPU times: user 27 s, sys: 1.11 s, total: 28.1 s\n",
      "Wall time: 12.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from functools import partial\n",
    "from uuid import uuid4\n",
    "\n",
    "from utils import rebatch_by_events\n",
    "\n",
    "datasets['train'] = rebatch_by_events(datasets['train'], batch_size=5040, date_column='date', nb_events_by_user_by_day=8)\n",
    "for key in ['val', 'test']:\n",
    "    datasets[key] = rebatch_by_events(datasets[key], batch_size=5040, date_column='date', nb_events_by_user_by_day=8,\n",
    "                                      seed=1729).cache(f'/tmp/{uuid4()}.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20173463-08c7-4b6f-a9b8-ade7c5c1121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_equal_weights\n",
    "\n",
    "for key in datasets:\n",
    "    datasets[key] = datasets[key].map(partial(add_equal_weights, features=offer_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10790714",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c685bae",
   "metadata": {},
   "source": [
    "First we need to get number of different modalities inputs can take from saved vectorizers (it will be used in embeddings layer definition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb20fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_inverse_lookups\n",
    "inverse_lookups = load_inverse_lookups(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a92d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "vocabulary_sizes = {}\n",
    "\n",
    "for feature in offer_features:\n",
    "    vocabulary_sizes[feature] = inverse_lookups[feature].vocabulary_size()\n",
    "\n",
    "for feature in user_features:\n",
    "    for key in inverse_lookups:\n",
    "        pattern = re.compile(r\"{}(\\w+)_{}\".format(AGG_PREFIX, key))\n",
    "        if pattern.match(feature):\n",
    "            vocabulary_sizes[feature] = vocabulary_sizes[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4c620",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fd6b2",
   "metadata": {},
   "source": [
    "Now we can assemble all these layers into final model. Note that offer compression weights and interaction kernels are shared between different augmentations we generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e60cfa",
   "metadata": {},
   "source": [
    "<img src=\"resources/group_by_augmentations_model.png\" alt=\"model\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8378e",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346c3a49-6895-42d8-86d3-167ba4b2a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100\n",
    "L1_COEFF = 4e-7\n",
    "DROPOUT = 0.1\n",
    "NB_AUGMENTATIONS = 3\n",
    "AVERAGE_NUMBER_OF_FEATURES_IN_AUGMENTATION = 2\n",
    "USER_META_FEATURES = 5\n",
    "OFFER_META_FEATURES = 3\n",
    "\n",
    "def REGULARIZER():\n",
    "    return {'class_name': 'L1L2', 'config': {'l1': L1_COEFF, 'l2': 0.}}\n",
    "\n",
    "def OUTPUT_DNN():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('tanh'),\n",
    "        tf.keras.layers.Dense(50,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('tanh'),\n",
    "        tf.keras.layers.Dense(1,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "    ], name='output_Dnn')\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "NUMBER_OF_NEGATIVES = 4\n",
    "LOSS = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "AUC_METRIC = tf.keras.metrics.AUC(from_logits=True)\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "OPTIMIZER = tfa.optimizers.AdamW(weight_decay=4e-8, learning_rate=0.0009)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f94016",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b103374",
   "metadata": {},
   "source": [
    "We will define embeddings with the same `WeightedEmbeddings` layer described in [the training of a simple model](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/training/movielens%20simple%20model.ipynb) with one addition:\n",
    "* for offer features when aggregating a list of embedding vectors, we will also calculate variance and not only mean vector\n",
    "\n",
    "It is easy to do in the same sparse-dense matrix multiplication operation as mean calculation (we get second moment and then calculate variance from it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca4935ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import get_input_layer, WeightedEmbeddings\n",
    "from utils import WEIGHT_SUFFIX\n",
    "\n",
    "inputs = {}\n",
    "embedded_user_features, embedded_offer_features, variance_offer_features = {}, {}, {}\n",
    "for feature in user_features:\n",
    "    inputs[feature] = get_input_layer(feature)\n",
    "    emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                   EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                   embeddings_regularizer=REGULARIZER())\n",
    "    embedded_user_features[feature] = emb_layer(inputs[feature])\n",
    "for feature in offer_features:\n",
    "    # for offer features we need weights:\n",
    "    # with dummy weights during training, and the ones used for a feature's averaging at inference time\n",
    "    inputs[f'{feature}_weight'] = get_input_layer(f'{feature}_weight', tf.float32)\n",
    "    inputs[feature] = get_input_layer(feature)\n",
    "    emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                   EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                   embeddings_regularizer=REGULARIZER(),\n",
    "                                   calculate_variance=True)\n",
    "    embedded_offer_features[feature], variance_offer_features[feature] =\\\n",
    "        emb_layer(inputs[feature], inputs[f'{feature}_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442c975",
   "metadata": {},
   "source": [
    "### Combining everything into model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4753b65",
   "metadata": {},
   "source": [
    "Now we can define described model architecture on the top of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "48eb9977",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stacked = tf.stack(list(embedded_user_features.values()), axis=1)\n",
    "offer_stacked = tf.stack(list(embedded_offer_features.values()), axis=1)\n",
    "offer_variance = tf.stack(list(variance_offer_features.values()), axis=1)\n",
    "stacked_raw_offer_attrs = tf.stack([tf.cast(inp.values, tf.int32) for feature, inp in inputs.items()\n",
    "                                    if feature in offer_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01bc2e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/initializers/initializers_v2.py:121: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  f\"The initializer {self.__class__.__name__} is unseeded \"\n"
     ]
    }
   ],
   "source": [
    "from layers import KeyGenerator, GroupBy\n",
    "from layers import UserFeaturesCompressor\n",
    "from layers import OfferFeaturesCompressor\n",
    "\n",
    "from layers import MaskNet\n",
    "\n",
    "from layers import BiLinearInteraction\n",
    "\n",
    "\n",
    "group_by = GroupBy(name='group_by')\n",
    "key_generator = KeyGenerator(number_of_offer_attributes=len(offer_features),\n",
    "                             average_number_of_attributes_in_key=AVERAGE_NUMBER_OF_FEATURES_IN_AUGMENTATION,\n",
    "                             name='grp_key_generator')\n",
    "\n",
    "user_compressed = UserFeaturesCompressor(USER_META_FEATURES, DROPOUT,\n",
    "                                         name='user_compressor')(user_stacked)\n",
    "offer_features_compressor = OfferFeaturesCompressor(OFFER_META_FEATURES, DROPOUT, name='offer_compressor')\n",
    "mask_net = MaskNet(OFFER_META_FEATURES, DROPOUT, name='mask_generation')\n",
    "apply_mask = tf.keras.layers.Multiply(name='apply_mask')\n",
    "bi_linear_interaction = BiLinearInteraction(number_of_negatives=NUMBER_OF_NEGATIVES, dropout_rate=DROPOUT,\n",
    "                                            initializer='random_normal', regularizer=REGULARIZER(),\n",
    "                                            name='interaction')\n",
    "output_dnn = OUTPUT_DNN()\n",
    "\n",
    "augmentation_predictions = []\n",
    "for i in range(NB_AUGMENTATIONS):\n",
    "    group_by_key = key_generator(stacked_raw_offer_attrs)\n",
    "    mean_offer_emb, variance_offer_emb = group_by(group_by_key, offer_stacked)\n",
    "    compressed_offer_embeddings = offer_features_compressor([mean_offer_emb, variance_offer_emb])\n",
    "    mask = mask_net([mean_offer_emb, variance_offer_emb])\n",
    "    masked_offer_embeddings = apply_mask([compressed_offer_embeddings, mask])\n",
    "    _output = output_dnn(bi_linear_interaction([user_compressed, masked_offer_embeddings], generate_negatives=True))\n",
    "    augmentation_predictions.append(_output)\n",
    "output = tf.concat(augmentation_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e5f2ba-237c-4559-9c1d-40a34508b78d",
   "metadata": {},
   "source": [
    "And for evaluation we don't need to create augmentations, we need just to take offer features' mean and variance coming from inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c4086eb-1451-4db7-abaa-b2eddd5d78b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_offer_embeddings = offer_features_compressor([offer_stacked, offer_variance])\n",
    "mask = mask_net([offer_stacked, offer_variance])\n",
    "masked_offer_embeddings = apply_mask([compressed_offer_embeddings, mask])\n",
    "\n",
    "eval_output = output_dnn(bi_linear_interaction([user_compressed, masked_offer_embeddings], generate_negatives=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2cc3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import BroadcastLoss, BroadcastMetric\n",
    "\n",
    "model = tf.keras.Model(inputs, output, name='group_by_augmentations')\n",
    "model.compile(optimizer=OPTIMIZER,\n",
    "              loss=BroadcastLoss(LOSS, NUMBER_OF_NEGATIVES),\n",
    "              metrics=[BroadcastMetric(AUC_METRIC, NUMBER_OF_NEGATIVES)])\n",
    "\n",
    "eval_model = tf.keras.Model(inputs, eval_output, name='group_by_augmentations_eval')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81c8b1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ce05bab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['user_id', 'date'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    682/Unknown - 647s 859ms/step - loss: 0.4643 - auc: 0.7114"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 22:17:48.991534: W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682/682 [==============================] - 709s 949ms/step - loss: 0.4643 - auc: 0.7114 - val_loss: 0.4425 - val_auc: 0.7324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-05 22:18:47.881811: W tensorflow/core/kernels/data/cache_dataset_ops.cc:296] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "682/682 [==============================] - 608s 891ms/step - loss: 0.4392 - auc: 0.7361 - val_loss: 0.4379 - val_auc: 0.7401\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 590s 864ms/step - loss: 0.4352 - auc: 0.7423 - val_loss: 0.4387 - val_auc: 0.7383\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 584s 856ms/step - loss: 0.4343 - auc: 0.7439 - val_loss: 0.4373 - val_auc: 0.7402\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 587s 860ms/step - loss: 0.4335 - auc: 0.7453 - val_loss: 0.4364 - val_auc: 0.7421\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 579s 848ms/step - loss: 0.4325 - auc: 0.7471 - val_loss: 0.4377 - val_auc: 0.7411\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 582s 853ms/step - loss: 0.4319 - auc: 0.7482 - val_loss: 0.4332 - val_auc: 0.7463\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 583s 855ms/step - loss: 0.4316 - auc: 0.7491 - val_loss: 0.4374 - val_auc: 0.7411\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 585s 857ms/step - loss: 0.4308 - auc: 0.7507 - val_loss: 0.4354 - val_auc: 0.7454\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 580s 850ms/step - loss: 0.4315 - auc: 0.7499 - val_loss: 0.4347 - val_auc: 0.7468\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f61f334a8d0>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datasets['train'], epochs=EPOCHS, validation_data=datasets['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27080ac6-2c87-49b0-b942-7c9bc02c29a0",
   "metadata": {},
   "source": [
    "## Single task models benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb00e26-0475-459a-9973-5aeb2fc8faf3",
   "metadata": {},
   "source": [
    "Using same approach as in [the simple model notebook](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/training/movielens%20simple%20model.ipynb) we will look at performance gap between the model with group by augmentations against set of models specialized on tasks corresponding to one offer feature at time. We won't use augmentations in those baseline models, because they will be already aligned with offer we will use in evaluation afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "373994d8-b6ec-4d7c-b878-b8c1f7bc58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offer columns we want to evaluate, specific to dataset we test\n",
    "TASKS = ['product_id', 'category1', 'category2', 'category3', 'brand', 'priceCluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2170ef73-1e06-4dda-9463-be8c1e71a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_linear_interaction_model(single_task_feature, name='bi_linear_model'):\n",
    "    # user_features, vocabulary_sizes, EMBEDDING_DIM, REGULARIZER, USER_TOWER, OFFER_TOWER,\n",
    "    # OPTIMIZER, LOSS, NUMBER_OF_NEGATIVES\n",
    "    # come from global scope, but can be passed as params instead\n",
    "    inputs = {}\n",
    "    embedded_user_features, embedded_offer_features, variance_offer_features = {}, {}, {}\n",
    "    for feature in user_features:\n",
    "        inputs[feature] = get_input_layer(feature)\n",
    "        emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                       EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                       embeddings_regularizer=REGULARIZER())\n",
    "        embedded_user_features[feature] = emb_layer(inputs[feature])\n",
    "\n",
    "    # for offer feature we need weights:\n",
    "    # with dummy weights during training, and the ones used for a feature's averaging at inference time\n",
    "    inputs[f'{single_task_feature}_weight'] = get_input_layer(f'{single_task_feature}_weight', tf.float32)\n",
    "    inputs[single_task_feature] = get_input_layer(single_task_feature)\n",
    "    emb_layer = WeightedEmbeddings(vocabulary_sizes[single_task_feature],\n",
    "                                   EMBEDDING_DIM, name=f'{single_task_feature}_embedding',\n",
    "                                   embeddings_regularizer=REGULARIZER())\n",
    "    embedded_offer_feature = emb_layer(inputs[single_task_feature],\n",
    "                                       inputs[f'{single_task_feature}_weight'])\n",
    "    \n",
    "    user_stacked = tf.stack(list(embedded_user_features.values()), axis=1)\n",
    "    offer_stacked = tf.expand_dims(embedded_offer_feature, axis=1)\n",
    "    \n",
    "    \n",
    "    user_compressed = UserFeaturesCompressor(USER_META_FEATURES, DROPOUT,\n",
    "                                             name='user_compressor')(user_stacked)\n",
    "    mask_net = MaskNet(OFFER_META_FEATURES, DROPOUT, name='mask_generation')\n",
    "    apply_mask = tf.keras.layers.Multiply(name='apply_mask')\n",
    "    bi_linear_interaction = BiLinearInteraction(number_of_negatives=NUMBER_OF_NEGATIVES, dropout_rate=DROPOUT,\n",
    "                                                initializer='random_normal', regularizer=REGULARIZER(),\n",
    "                                                name='interaction')\n",
    "    output_dnn = OUTPUT_DNN()\n",
    "\n",
    "    \n",
    "    mask = mask_net([offer_stacked, offer_stacked])\n",
    "    masked_offer_embeddings = apply_mask([offer_stacked, mask])\n",
    "    \n",
    "    output = OUTPUT_DNN()(bi_linear_interaction([user_compressed, masked_offer_embeddings],\n",
    "                                                generate_negatives=True))\n",
    "\n",
    "    model = tf.keras.Model(inputs, output, name=name)\n",
    "    model.compile(optimizer=OPTIMIZER,\n",
    "                  loss=BroadcastLoss(LOSS, NUMBER_OF_NEGATIVES),\n",
    "                  metrics=[BroadcastMetric(AUC_METRIC, NUMBER_OF_NEGATIVES)])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6b37f-b971-41b6-bc36-e60974bcdb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['user_id', 'category2', 'category1', 'brand', 'category3', 'priceCluster', 'date', 'category2_weight', 'category1_weight', 'brand_weight', 'category3_weight', 'priceCluster_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682/682 [==============================] - 331s 439ms/step - loss: 0.4645 - auc: 0.7330 - val_loss: 0.4477 - val_auc: 0.7521\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 289s 423ms/step - loss: 0.4397 - auc: 0.7602 - val_loss: 0.4407 - val_auc: 0.7577\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 288s 422ms/step - loss: 0.4327 - auc: 0.7685 - val_loss: 0.4396 - val_auc: 0.7592\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 284s 416ms/step - loss: 0.4292 - auc: 0.7736 - val_loss: 0.4410 - val_auc: 0.7596\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 293s 429ms/step - loss: 0.4267 - auc: 0.7776 - val_loss: 0.4403 - val_auc: 0.7602\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 292s 428ms/step - loss: 0.4251 - auc: 0.7809 - val_loss: 0.4413 - val_auc: 0.7599\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 289s 423ms/step - loss: 0.4238 - auc: 0.7839 - val_loss: 0.4467 - val_auc: 0.7593\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 290s 425ms/step - loss: 0.4227 - auc: 0.7865 - val_loss: 0.4441 - val_auc: 0.7581\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 290s 425ms/step - loss: 0.4220 - auc: 0.7888 - val_loss: 0.4462 - val_auc: 0.7579\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 287s 420ms/step - loss: 0.4213 - auc: 0.7912 - val_loss: 0.4499 - val_auc: 0.7572\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['user_id', 'category2', 'product_id', 'brand', 'category3', 'priceCluster', 'date', 'category2_weight', 'product_id_weight', 'brand_weight', 'category3_weight', 'priceCluster_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682/682 [==============================] - 337s 449ms/step - loss: 0.4789 - auc: 0.6784 - val_loss: 0.4716 - val_auc: 0.6728\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 288s 422ms/step - loss: 0.4714 - auc: 0.6721 - val_loss: 0.4707 - val_auc: 0.6739\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 282s 413ms/step - loss: 0.4701 - auc: 0.6734 - val_loss: 0.4696 - val_auc: 0.6743\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 278s 408ms/step - loss: 0.4693 - auc: 0.6744 - val_loss: 0.4692 - val_auc: 0.6744\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 289s 423ms/step - loss: 0.4688 - auc: 0.6753 - val_loss: 0.4692 - val_auc: 0.6745\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 291s 426ms/step - loss: 0.4683 - auc: 0.6763 - val_loss: 0.4691 - val_auc: 0.6748\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 284s 416ms/step - loss: 0.4679 - auc: 0.6773 - val_loss: 0.4695 - val_auc: 0.6739\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 287s 421ms/step - loss: 0.4676 - auc: 0.6783 - val_loss: 0.4694 - val_auc: 0.6743\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 286s 419ms/step - loss: 0.4674 - auc: 0.6792 - val_loss: 0.4701 - val_auc: 0.6741\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 291s 425ms/step - loss: 0.4672 - auc: 0.6801 - val_loss: 0.4704 - val_auc: 0.6739\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['user_id', 'product_id', 'category1', 'brand', 'category3', 'priceCluster', 'date', 'product_id_weight', 'category1_weight', 'brand_weight', 'category3_weight', 'priceCluster_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682/682 [==============================] - 338s 444ms/step - loss: 0.4722 - auc: 0.6768 - val_loss: 0.4638 - val_auc: 0.6877\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 290s 425ms/step - loss: 0.4629 - auc: 0.6873 - val_loss: 0.4621 - val_auc: 0.6891\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 286s 419ms/step - loss: 0.4614 - auc: 0.6890 - val_loss: 0.4617 - val_auc: 0.6894\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 280s 410ms/step - loss: 0.4605 - auc: 0.6903 - val_loss: 0.4610 - val_auc: 0.6899\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 285s 417ms/step - loss: 0.4600 - auc: 0.6912 - val_loss: 0.4612 - val_auc: 0.6901\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 280s 410ms/step - loss: 0.4594 - auc: 0.6925 - val_loss: 0.4613 - val_auc: 0.6897\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 279s 409ms/step - loss: 0.4590 - auc: 0.6935 - val_loss: 0.4616 - val_auc: 0.6899\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 288s 419ms/step - loss: 0.4586 - auc: 0.6948 - val_loss: 0.4616 - val_auc: 0.6896\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 284s 417ms/step - loss: 0.4584 - auc: 0.6958 - val_loss: 0.4626 - val_auc: 0.6892\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 291s 426ms/step - loss: 0.4582 - auc: 0.6969 - val_loss: 0.4623 - val_auc: 0.6891\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['user_id', 'category2', 'product_id', 'category1', 'brand', 'priceCluster', 'date', 'category2_weight', 'product_id_weight', 'category1_weight', 'brand_weight', 'priceCluster_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682/682 [==============================] - 347s 458ms/step - loss: 0.4742 - auc: 0.6732 - val_loss: 0.4655 - val_auc: 0.6816\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 290s 424ms/step - loss: 0.4645 - auc: 0.6816 - val_loss: 0.4638 - val_auc: 0.6834\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 283s 414ms/step - loss: 0.4629 - auc: 0.6836 - val_loss: 0.4632 - val_auc: 0.6843\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 296s 434ms/step - loss: 0.4620 - auc: 0.6849 - val_loss: 0.4630 - val_auc: 0.6828\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 293s 429ms/step - loss: 0.4614 - auc: 0.6861 - val_loss: 0.4629 - val_auc: 0.6848\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 304s 445ms/step - loss: 0.4611 - auc: 0.6869 - val_loss: 0.4628 - val_auc: 0.6850\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 308s 452ms/step - loss: 0.4607 - auc: 0.6880 - val_loss: 0.4625 - val_auc: 0.6847\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 296s 434ms/step - loss: 0.4604 - auc: 0.6890 - val_loss: 0.4628 - val_auc: 0.6850\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 292s 428ms/step - loss: 0.4601 - auc: 0.6901 - val_loss: 0.4637 - val_auc: 0.6846\n",
      "Epoch 10/10\n",
      "682/682 [==============================] - 290s 425ms/step - loss: 0.4599 - auc: 0.6911 - val_loss: 0.4638 - val_auc: 0.6847\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['user_id', 'category2', 'product_id', 'category1', 'category3', 'priceCluster', 'date', 'category2_weight', 'product_id_weight', 'category1_weight', 'category3_weight', 'priceCluster_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "682/682 [==============================] - 338s 446ms/step - loss: 0.4584 - auc: 0.7127 - val_loss: 0.4465 - val_auc: 0.7316\n",
      "Epoch 2/10\n",
      "682/682 [==============================] - 290s 424ms/step - loss: 0.4445 - auc: 0.7331 - val_loss: 0.4432 - val_auc: 0.7347\n",
      "Epoch 3/10\n",
      "682/682 [==============================] - 285s 418ms/step - loss: 0.4418 - auc: 0.7364 - val_loss: 0.4421 - val_auc: 0.7360\n",
      "Epoch 4/10\n",
      "682/682 [==============================] - 287s 419ms/step - loss: 0.4404 - auc: 0.7383 - val_loss: 0.4415 - val_auc: 0.7359\n",
      "Epoch 5/10\n",
      "682/682 [==============================] - 289s 424ms/step - loss: 0.4394 - auc: 0.7398 - val_loss: 0.4413 - val_auc: 0.7363\n",
      "Epoch 6/10\n",
      "682/682 [==============================] - 290s 425ms/step - loss: 0.4386 - auc: 0.7412 - val_loss: 0.4414 - val_auc: 0.7371\n",
      "Epoch 7/10\n",
      "682/682 [==============================] - 290s 425ms/step - loss: 0.4381 - auc: 0.7424 - val_loss: 0.4416 - val_auc: 0.7371\n",
      "Epoch 8/10\n",
      "682/682 [==============================] - 287s 420ms/step - loss: 0.4376 - auc: 0.7434 - val_loss: 0.4419 - val_auc: 0.7365\n",
      "Epoch 9/10\n",
      "682/682 [==============================] - 287s 420ms/step - loss: 0.4373 - auc: 0.7446 - val_loss: 0.4426 - val_auc: 0.7366\n",
      "Epoch 10/10\n",
      "582/682 [========================>.....] - ETA: 40s - loss: 0.4370 - auc: 0.7457"
     ]
    }
   ],
   "source": [
    "mono_feature_models = {}\n",
    "for task_offer_feature in TASKS:\n",
    "    mono_feature_models[task_offer_feature] =\\\n",
    "        bi_linear_interaction_model(task_offer_feature, name=f'{task_offer_feature}_model')\n",
    "    mono_feature_models[task_offer_feature].fit(datasets['train'],\n",
    "                                                epochs=EPOCHS,\n",
    "                                                validation_data=datasets['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc7d11-391a-4466-9bb5-bf9f2bbcc203",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb861dcc-1c7c-4a3e-be52-864c97543ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from utils import prepare_single_task_dataset\n",
    "test_datasets = {}\n",
    "for task_offer_feature in TASKS:\n",
    "    test_datasets[task_offer_feature] = \\\n",
    "        prepare_single_task_dataset(datasets['test'], task_offer_feature, offer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644699ac-98a0-496d-8a76-afb73852bc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "from utils import evaluate_model, wAUC\n",
    "\n",
    "aucs = defaultdict(dict)\n",
    "for task_offer_feature in TASKS:\n",
    "    for model_name in TASKS:\n",
    "        aucs[task_offer_feature][f'MONO:{model_name}'] = \\\n",
    "            evaluate_model(mono_feature_models[model_name],\n",
    "                           task_offer_feature, test_datasets, inverse_lookups, NUMBER_OF_NEGATIVES)\n",
    "    aucs[task_offer_feature]['new model'] = \\\n",
    "            evaluate_model(eval_model, task_offer_feature, test_datasets, inverse_lookups, NUMBER_OF_NEGATIVES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f862e7c9-c599-408c-8572-d2f924a36806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.DataFrame()\n",
    "for task_name in aucs:\n",
    "    for model_name in aucs[task_name]:\n",
    "        w_auc = wAUC(aucs[task_name][model_name])\n",
    "        results = results.append({'wAUC': w_auc, 'offers': task_name, 'model': model_name}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ec9ac730-c8f3-4a95-8102-a2551742f98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_51e2f_row0_col0, #T_51e2f_row1_col1, #T_51e2f_row2_col2, #T_51e2f_row3_col3, #T_51e2f_row4_col4, #T_51e2f_row5_col5, #T_51e2f_row6_col5 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row0_col1 {\n",
       "  background-color: #6788ee;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row0_col2 {\n",
       "  background-color: #82a6fb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row0_col3 {\n",
       "  background-color: #5572df;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row0_col4 {\n",
       "  background-color: #90b2fe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row0_col5 {\n",
       "  background-color: #f7b89c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row1_col0, #T_51e2f_row3_col4, #T_51e2f_row3_col5, #T_51e2f_row4_col1, #T_51e2f_row4_col2, #T_51e2f_row4_col3 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row1_col2 {\n",
       "  background-color: #ea7b60;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row1_col3 {\n",
       "  background-color: #f3c8b2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row1_col4 {\n",
       "  background-color: #3f53c6;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row1_col5 {\n",
       "  background-color: #3c4ec2;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row2_col0 {\n",
       "  background-color: #4358cb;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row2_col1 {\n",
       "  background-color: #f08b6e;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row2_col3 {\n",
       "  background-color: #f7aa8c;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row2_col4 {\n",
       "  background-color: #3d50c3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row2_col5, #T_51e2f_row3_col0 {\n",
       "  background-color: #4a63d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row3_col1 {\n",
       "  background-color: #f1cdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row3_col2 {\n",
       "  background-color: #f6bda2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row4_col0 {\n",
       "  background-color: #779af7;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row4_col5 {\n",
       "  background-color: #efcebd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row5_col0 {\n",
       "  background-color: #f5a081;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row5_col1 {\n",
       "  background-color: #dfdbd9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row5_col2 {\n",
       "  background-color: #cfdaea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row5_col3 {\n",
       "  background-color: #f2cab5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row5_col4 {\n",
       "  background-color: #f3c7b1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row6_col0 {\n",
       "  background-color: #e0654f;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row6_col1 {\n",
       "  background-color: #e16751;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_51e2f_row6_col2 {\n",
       "  background-color: #f7a98b;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row6_col3 {\n",
       "  background-color: #f7b599;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_51e2f_row6_col4 {\n",
       "  background-color: #ee8669;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_51e2f_\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >offers</th>\n",
       "      <th class=\"col_heading level0 col0\" >brand</th>\n",
       "      <th class=\"col_heading level0 col1\" >category1</th>\n",
       "      <th class=\"col_heading level0 col2\" >category2</th>\n",
       "      <th class=\"col_heading level0 col3\" >category3</th>\n",
       "      <th class=\"col_heading level0 col4\" >priceCluster</th>\n",
       "      <th class=\"col_heading level0 col5\" >product_id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_51e2f_level0_row0\" class=\"row_heading level0 row0\" >MONO:brand</th>\n",
       "      <td id=\"T_51e2f_row0_col0\" class=\"data row0 col0\" >0.729320</td>\n",
       "      <td id=\"T_51e2f_row0_col1\" class=\"data row0 col1\" >0.584560</td>\n",
       "      <td id=\"T_51e2f_row0_col2\" class=\"data row0 col2\" >0.602323</td>\n",
       "      <td id=\"T_51e2f_row0_col3\" class=\"data row0 col3\" >0.592103</td>\n",
       "      <td id=\"T_51e2f_row0_col4\" class=\"data row0 col4\" >0.601051</td>\n",
       "      <td id=\"T_51e2f_row0_col5\" class=\"data row0 col5\" >0.730183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_51e2f_level0_row1\" class=\"row_heading level0 row1\" >MONO:category1</th>\n",
       "      <td id=\"T_51e2f_row1_col0\" class=\"data row1 col0\" >0.622370</td>\n",
       "      <td id=\"T_51e2f_row1_col1\" class=\"data row1 col1\" >0.665993</td>\n",
       "      <td id=\"T_51e2f_row1_col2\" class=\"data row1 col2\" >0.662037</td>\n",
       "      <td id=\"T_51e2f_row1_col3\" class=\"data row1 col3\" >0.639652</td>\n",
       "      <td id=\"T_51e2f_row1_col4\" class=\"data row1 col4\" >0.568841</td>\n",
       "      <td id=\"T_51e2f_row1_col5\" class=\"data row1 col5\" >0.657654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_51e2f_level0_row2\" class=\"row_heading level0 row2\" >MONO:category2</th>\n",
       "      <td id=\"T_51e2f_row2_col0\" class=\"data row2 col0\" >0.625577</td>\n",
       "      <td id=\"T_51e2f_row2_col1\" class=\"data row2 col1\" >0.645309</td>\n",
       "      <td id=\"T_51e2f_row2_col2\" class=\"data row2 col2\" >0.680136</td>\n",
       "      <td id=\"T_51e2f_row2_col3\" class=\"data row2 col3\" >0.648451</td>\n",
       "      <td id=\"T_51e2f_row2_col4\" class=\"data row2 col4\" >0.567752</td>\n",
       "      <td id=\"T_51e2f_row2_col5\" class=\"data row2 col5\" >0.662823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_51e2f_level0_row3\" class=\"row_heading level0 row3\" >MONO:category3</th>\n",
       "      <td id=\"T_51e2f_row3_col0\" class=\"data row3 col0\" >0.627817</td>\n",
       "      <td id=\"T_51e2f_row3_col1\" class=\"data row3 col1\" >0.627434</td>\n",
       "      <td id=\"T_51e2f_row3_col2\" class=\"data row3 col2\" >0.645139</td>\n",
       "      <td id=\"T_51e2f_row3_col3\" class=\"data row3 col3\" >0.675197</td>\n",
       "      <td id=\"T_51e2f_row3_col4\" class=\"data row3 col4\" >0.566485</td>\n",
       "      <td id=\"T_51e2f_row3_col5\" class=\"data row3 col5\" >0.657189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_51e2f_level0_row4\" class=\"row_heading level0 row4\" >MONO:priceCluster</th>\n",
       "      <td id=\"T_51e2f_row4_col0\" class=\"data row4 col0\" >0.642769</td>\n",
       "      <td id=\"T_51e2f_row4_col1\" class=\"data row4 col1\" >0.571176</td>\n",
       "      <td id=\"T_51e2f_row4_col2\" class=\"data row4 col2\" >0.580199</td>\n",
       "      <td id=\"T_51e2f_row4_col3\" class=\"data row4 col3\" >0.584151</td>\n",
       "      <td id=\"T_51e2f_row4_col4\" class=\"data row4 col4\" >0.699928</td>\n",
       "      <td id=\"T_51e2f_row4_col5\" class=\"data row4 col5\" >0.721428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_51e2f_level0_row5\" class=\"row_heading level0 row5\" >MONO:product_id</th>\n",
       "      <td id=\"T_51e2f_row5_col0\" class=\"data row5 col0\" >0.700882</td>\n",
       "      <td id=\"T_51e2f_row5_col1\" class=\"data row5 col1\" >0.619597</td>\n",
       "      <td id=\"T_51e2f_row5_col2\" class=\"data row5 col2\" >0.625432</td>\n",
       "      <td id=\"T_51e2f_row5_col3\" class=\"data row5 col3\" >0.639249</td>\n",
       "      <td id=\"T_51e2f_row5_col4\" class=\"data row5 col4\" >0.648758</td>\n",
       "      <td id=\"T_51e2f_row5_col5\" class=\"data row5 col5\" >0.766634</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_51e2f_level0_row6\" class=\"row_heading level0 row6\" >new model</th>\n",
       "      <td id=\"T_51e2f_row6_col0\" class=\"data row6 col0\" >0.714909</td>\n",
       "      <td id=\"T_51e2f_row6_col1\" class=\"data row6 col1\" >0.652915</td>\n",
       "      <td id=\"T_51e2f_row6_col2\" class=\"data row6 col2\" >0.651079</td>\n",
       "      <td id=\"T_51e2f_row6_col3\" class=\"data row6 col3\" >0.645676</td>\n",
       "      <td id=\"T_51e2f_row6_col4\" class=\"data row6 col4\" >0.672518</td>\n",
       "      <td id=\"T_51e2f_row6_col5\" class=\"data row6 col5\" >0.766951</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f61385d9990>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(results, 'wAUC', 'model', 'offers').style.background_gradient(cmap='coolwarm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [moksha-tf2-cpu.2-7] (Local)",
   "language": "python",
   "name": "local-eu.gcr.io_tinyclues-experiments_tinyclues_moksha-tf2-cpu.2-7_latest__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
