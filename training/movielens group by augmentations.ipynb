{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea932e5b",
   "metadata": {},
   "source": [
    "# Bi-linear interaction model with group-by augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6f05f",
   "metadata": {},
   "source": [
    "## Prepare dataset for training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96897ae",
   "metadata": {},
   "source": [
    "Following the same steps as in [the training of a simple model](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/training/movielens%20simple%20model.ipynb), we first load splitted dataset generated in [notebook](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/dataset_preprocessing/movielens%20with%20imdb.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1899e738-56d3-455f-afcd-ce80fcb17232",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'movielens_imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19cc05cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jupyter/recsys-multi-atrribute-benchmark/training/utils.py:26: load (from tensorflow.python.data.experimental.ops.io) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.load(...)` instead.\n"
     ]
    }
   ],
   "source": [
    "from utils import load_dataset\n",
    "\n",
    "datasets = {}\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    datasets[split_name] = load_dataset(DATASET, split_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d1b29e",
   "metadata": {},
   "source": [
    "Then we parse features' names to obtain a list of offer features (that will be used to modelize film) and a list of user features (aggregated history up to chosen date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5077ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import AGG_PREFIX\n",
    "\n",
    "all_columns = list(datasets['train'].element_spec.keys())\n",
    "technical_columns = ['userId', 'date']\n",
    "user_features = list(filter(lambda x: x.startswith(AGG_PREFIX), all_columns))\n",
    "offer_features = list(filter(lambda x: x not in user_features + technical_columns, all_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d5781",
   "metadata": {},
   "source": [
    "### Rebatching datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96990cb",
   "metadata": {},
   "source": [
    "Splitting dataset into smaller batches in the same way as described in [the training of a simple model](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/training/movielens%20simple%20model.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cfa53a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n",
      "CPU times: user 39.2 s, sys: 6.2 s, total: 45.4 s\n",
      "Wall time: 33 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from functools import partial\n",
    "from uuid import uuid4\n",
    "\n",
    "from utils import rebatch_by_events\n",
    "\n",
    "datasets['train'] = rebatch_by_events(datasets['train'], batch_size=10080, date_column='date', nb_events_by_user_by_day=8)\n",
    "for key in ['val', 'test']:\n",
    "    datasets[key] = rebatch_by_events(datasets[key], batch_size=50400, date_column='date', nb_events_by_user_by_day=8,\n",
    "                                      seed=1729).cache(f'/tmp/{uuid4()}.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20173463-08c7-4b6f-a9b8-ade7c5c1121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_equal_weights\n",
    "\n",
    "for key in datasets:\n",
    "    datasets[key] = datasets[key].map(partial(add_equal_weights, features=offer_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10790714",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c685bae",
   "metadata": {},
   "source": [
    "First we need to get number of different modalities inputs can take from saved vectorizers (it will be used in embeddings layer definition):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb20fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_inverse_lookups\n",
    "inverse_lookups = load_inverse_lookups(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a92d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "vocabulary_sizes = {}\n",
    "\n",
    "for feature in offer_features:\n",
    "    vocabulary_sizes[feature] = inverse_lookups[feature].vocabulary_size()\n",
    "\n",
    "for feature in user_features:\n",
    "    for key in inverse_lookups:\n",
    "        pattern = re.compile(r\"{}(\\w+)_{}\".format(AGG_PREFIX, key))\n",
    "        if pattern.match(feature):\n",
    "            vocabulary_sizes[feature] = vocabulary_sizes[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9119b378",
   "metadata": {},
   "source": [
    "### Layers definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9643942",
   "metadata": {},
   "source": [
    "To define a model with group-by augmentations we need to create some layers described in article (TODO link). In all following schemas we color weights that will be learned during training in red."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bc288f",
   "metadata": {},
   "source": [
    "#### Generation of group-by augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37870c44",
   "metadata": {},
   "source": [
    "To get group-by augmentations we need first choose randomly some offer features we will use to get group by keys that will consist of AND and OR combinations of values from chosen features. It is implemented in `KeyGenerator` layer. Next once we get keys, we will group by and calculate mean and variance embeddings vectors for other features wrt to chosen keys. Finally, we will broadcast mean and variance vectors back to return to original batch size. Both calculation and broadcast are implemented in `GroupBy` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1753e74f-9f8b-4ed5-aa0a-cc0a829794ef",
   "metadata": {},
   "source": [
    "<img src=\"resources/group_by_augmentation.png\" alt=\"group-by augmentation generation\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35350305",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from layers import KeyGenerator, GroupBy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0823204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_key_generator = KeyGenerator(number_of_offer_attributes=len(offer_features),\n",
    "                                  average_number_of_attributes_in_key=2,\n",
    "                                  name='test_key_generator')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b348b68d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10, 7), dtype=int32, numpy=\n",
       "array([[1, 4, 2, 0, 1, 0, 0],\n",
       "       [0, 0, 4, 2, 1, 4, 4],\n",
       "       [1, 3, 2, 2, 0, 4, 1],\n",
       "       [2, 4, 4, 3, 2, 3, 4],\n",
       "       [0, 3, 1, 3, 2, 2, 1],\n",
       "       [1, 1, 3, 2, 4, 3, 1],\n",
       "       [1, 2, 1, 2, 2, 4, 2],\n",
       "       [4, 4, 4, 0, 3, 0, 2],\n",
       "       [3, 2, 1, 1, 3, 4, 2],\n",
       "       [2, 1, 3, 4, 1, 1, 4]], dtype=int32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sampling randomly values for offer features\n",
    "test_offer_features = tf.random.uniform((10, len(offer_features)), maxval=5, dtype=tf.int32)\n",
    "test_offer_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "998c2da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 1, 2, 3, 4, 5, 2, 6, 7, 8], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# key generator returns hashed keys for group by\n",
    "test_keys = test_key_generator(test_offer_features)\n",
    "test_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c7598fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by = GroupBy(name='group_by')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "314d05c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have some embeddings vectors\n",
    "test_embeddings = tf.random.normal((10, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ffa681f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([10, 3]), TensorShape([10, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can apply group-by operation for all features\n",
    "test_mean, test_var = group_by(test_keys, test_embeddings)\n",
    "test_mean.shape, test_var.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "30527f8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3, 3), dtype=float64, numpy=\n",
       " array([[0.5, 0.5, 0. ],\n",
       "        [0.5, 0.5, 0. ],\n",
       "        [0. , 0. , 1. ]])>,\n",
       " <tf.Tensor: shape=(3, 3), dtype=float64, numpy=\n",
       " array([[0.25, 0.25, 0.  ],\n",
       "        [0.25, 0.25, 0.  ],\n",
       "        [0.  , 0.  , 0.  ]])>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or more direct example of group by\n",
    "import numpy as np\n",
    "group_by([0, 0, 1], np.eye(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43411cf",
   "metadata": {},
   "source": [
    "#### Compression of user features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1af9d9f",
   "metadata": {},
   "source": [
    "For optimisation of calculation of interaction we want to reduce number of user features we use. For that we will generate meta features using a sequence of fully connected layers based on `tf.keras.layers.experimental.EinsumDense` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e1d1267",
   "metadata": {},
   "source": [
    "<img src=\"resources/user_features_compression.png\" alt=\"compression of user features\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98942bed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 2, 7])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from layers import UserFeaturesCompressor\n",
    "test_user_compressor = UserFeaturesCompressor(number_of_meta_features=2,\n",
    "                                              dropout_rate=0.1,\n",
    "                                              name='test_user_compressor')\n",
    "test_user_compressor(tf.random.normal((10, 3, 7))).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668ff767",
   "metadata": {},
   "source": [
    "#### Compression of offer features and MaskNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "436c62da",
   "metadata": {},
   "source": [
    "This is a key layer that will create some meta offer features and apply instance guided mask over embedding dimension. For meta features, the idea is similar to user side: we want to get smaller number of features before interaction, but here using information about variance we can completely deactivate some features, depending on offer we want to predict."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f39e8f",
   "metadata": {},
   "source": [
    "<img src=\"resources/offer_features_compression.png\" alt=\"compression of offer features\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fd188a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 2, 7])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from layers import OfferFeaturesCompressor\n",
    "test_offer_compressor = OfferFeaturesCompressor(number_of_meta_features=2,\n",
    "                                                dropout_rate=0.1,\n",
    "                                                name='test_offer_compressor')\n",
    "test_offer_compressor([tf.random.normal((10, 3, 7)), tf.random.normal((10, 3, 7))]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8acd338e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 2, 7])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from layers import MaskNet\n",
    "test_mask = MaskNet(number_of_meta_features=2, dropout_rate=0.1)\n",
    "test_mask([tf.random.normal((10, 3, 7)), tf.random.normal((10, 3, 7))]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86ffb69",
   "metadata": {},
   "source": [
    "#### Bi-linear feature-wise interaction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45cf6525",
   "metadata": {},
   "source": [
    "Last step is a calculation of interaction using bi-linear kernel for each pair of meta features from user and from offer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7522b5",
   "metadata": {},
   "source": [
    "<img src=\"resources/bi_linear_interaction.png\" alt=\"bi-linear feature wise interaction\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127052be-6506-4507-83a3-d526c617c02c",
   "metadata": {},
   "source": [
    "We also incorporate mini-batch generation of negative examples inside this layer in the similar way described in [the training of a simple model](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/training/movielens%20simple%20model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2202cde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer RandomUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TensorShape([10, 12])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from layers import BiLinearInteraction\n",
    "test_interaction = BiLinearInteraction(number_of_negatives=3, dropout_rate=0., name='test_interaction')\n",
    "test_interaction([tf.random.normal((10, 4, 7)), tf.random.normal((10, 3, 5))], generate_negatives=False).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "41ea0f44-0144-4079-84c1-6d03464a99c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([48, 12])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_interaction([tf.random.normal((12, 4, 7)), tf.random.normal((12, 3, 5))], generate_negatives=True).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e4c620",
   "metadata": {},
   "source": [
    "### Model architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357fd6b2",
   "metadata": {},
   "source": [
    "Now we can assemble all these layers into final model. Note that offer compression weights and interaction kernels are shared between different augmentations we generate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e60cfa",
   "metadata": {},
   "source": [
    "<img src=\"resources/group_by_augmentations_model.png\" alt=\"model\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b8378e",
   "metadata": {},
   "source": [
    "### Model parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73764c02",
   "metadata": {},
   "source": [
    "For model's regularization we used a combination of several strategies:\n",
    "* `weight_decay` in an optimizer (for L2-penalty)\n",
    "* explicit L1-penalty on embedding layers\n",
    "* dropouts in fully-connected layers (after interaction, inside compression)\n",
    "\n",
    "We use `AdamW` optimizer and `BCE` loss, but in some cases it maybe be interesting to use [`FocalLoss`](https://www.tensorflow.org/addons/api_docs/python/tfa/losses/SigmoidFocalCrossEntropy) (with $\\gamma=1.5~..~2.5$) that automatically will concentrate on harder examples.\n",
    "\n",
    "There are some model parameters that can be changed (and tuned), during experimentations we found some typical values for those parameters:\n",
    "\n",
    "| parameter                                    | description                     | typical values | comment                                                                                                   |\n",
    "|----------------------------------------------|---------------------------------|----------------|-----------------------------------------------------------------------------------------------------------|\n",
    "| batch size, set above                        | batch size                                       | 5k … 20k       | it should not be too low if we want to have on-the-fly group-by                                      |\n",
    "| learning rate inside `OPTIMIZER`             | learning rate                                    | 0.001 .. 0.005 | usually we set it as a half of the learning rate used in standard training                                               |\n",
    "| `USER_META_FEATURES`, `OFFER_META_FEATURES`  | compression meta dimension                       | 2 .. 6         | prefer bigger values for larger number of offer features and complex (non-hierarchical) feature structure|\n",
    "| `NB_AUGMENTATIONS`                           | number of augmentations per step                 | 3 .. 10        | bigger for larger number of offer features                                                               |\n",
    "| `AVERAGE_NUMBER_OF_FEATURES_IN_AUGMENTATION` | how many offer features used for group by key    | 1.5 .. 3       | bigger for larger number of offer features                                                               |\n",
    "| `EPOCHS`                                     | number of epochs                                 | 2 - 50         | we need to double or triple number of epochs compared to std training                                      |\n",
    "| `EMBEDDING_DIM`                              | embedding latent dimensions                      | 15 … 60        | usually depends on the data amount and features modularity                                             |\n",
    "| `NUMBER_OF_NEGATIVES`                        | number of negatives examples                     | 3 ... 10       | bigger number of negative examples may create some collisions for higher level offers                    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1cf92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 30\n",
    "L1_COEFF = 1e-6\n",
    "DROPOUT = 0.05\n",
    "NB_AUGMENTATIONS = 3\n",
    "AVERAGE_NUMBER_OF_FEATURES_IN_AUGMENTATION = 2\n",
    "USER_META_FEATURES = 5\n",
    "OFFER_META_FEATURES = 3\n",
    "\n",
    "def REGULARIZER():\n",
    "    return {'class_name': 'L1L2', 'config': {'l1': L1_COEFF, 'l2': 0.}}\n",
    "\n",
    "def OUTPUT_DNN():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Dense(20, activation='gelu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(20, activation='gelu'),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Dense(1),\n",
    "    ])\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "NUMBER_OF_NEGATIVES = 4\n",
    "LOSS = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "AUC_METRIC = tf.keras.metrics.AUC(from_logits=True)\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "OPTIMIZER = tfa.optimizers.AdamW(weight_decay=1e-8, learning_rate=0.002)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f94016",
   "metadata": {},
   "source": [
    "### Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b103374",
   "metadata": {},
   "source": [
    "We will define embeddings with the same `WeightedEmbeddings` layer described in [the training of a simple model](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/training/movielens%20simple%20model.ipynb) with one addition:\n",
    "* for offer features when aggregating a list of embedding vectors, we will also calculate variance and not only mean vector\n",
    "\n",
    "It is easy to do in the same sparse-dense matrix multiplication operation as mean calculation (we get second moment and then calculate variance from it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca4935ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import get_input_layer, WeightedEmbeddings\n",
    "from utils import WEIGHT_SUFFIX\n",
    "\n",
    "inputs = {}\n",
    "embedded_user_features, embedded_offer_features, variance_offer_features = {}, {}, {}\n",
    "for feature in user_features:\n",
    "    inputs[feature] = get_input_layer(feature)\n",
    "    emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                   EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                   embeddings_regularizer=REGULARIZER())\n",
    "    embedded_user_features[feature] = emb_layer(inputs[feature])\n",
    "for feature in offer_features:\n",
    "    # for offer features we need weights:\n",
    "    # with dummy weights during training, and the ones used for a feature's averaging at inference time\n",
    "    inputs[f'{feature}_weight'] = get_input_layer(f'{feature}_weight', tf.float32)\n",
    "    inputs[feature] = get_input_layer(feature)\n",
    "    emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                   EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                   embeddings_regularizer=REGULARIZER(),\n",
    "                                   calculate_variance=True)\n",
    "    embedded_offer_features[feature], variance_offer_features[feature] =\\\n",
    "        emb_layer(inputs[feature], inputs[f'{feature}_weight'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f442c975",
   "metadata": {},
   "source": [
    "### Combining everything into model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4753b65",
   "metadata": {},
   "source": [
    "Now we can define described model architecture on the top of embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "48eb9977",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_stacked = tf.stack(list(embedded_user_features.values()), axis=1)\n",
    "offer_stacked = tf.stack(list(embedded_offer_features.values()), axis=1)\n",
    "offer_variance = tf.stack(list(variance_offer_features.values()), axis=1)\n",
    "stacked_raw_offer_attrs = tf.stack([tf.cast(inp.values, tf.int32) for feature, inp in inputs.items()\n",
    "                                    if feature in offer_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ea47cb93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KerasTensor: shape=(None, 7) dtype=int32 (created by layer 'tf.stack_3')>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stacked_raw_offer_attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01bc2e50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "key_generator = KeyGenerator(number_of_offer_attributes=len(offer_features),\n",
    "                             average_number_of_attributes_in_key=AVERAGE_NUMBER_OF_FEATURES_IN_AUGMENTATION,\n",
    "                             name='grp_key_generator')\n",
    "\n",
    "user_compressed = UserFeaturesCompressor(USER_META_FEATURES, DROPOUT,\n",
    "                                         name='user_compressor')(user_stacked)\n",
    "offer_features_compressor = OfferFeaturesCompressor(OFFER_META_FEATURES, DROPOUT, name='offer_compressor')\n",
    "mask_net = MaskNet(OFFER_META_FEATURES, DROPOUT, name='mask_generation')\n",
    "apply_mask = tf.keras.layers.Multiply(name='apply_mask')\n",
    "bi_linear_interaction = BiLinearInteraction(number_of_negatives=NUMBER_OF_NEGATIVES, dropout_rate=DROPOUT,\n",
    "                                            initializer='random_normal', regularizer=REGULARIZER(),\n",
    "                                            name='interaction')\n",
    "output_dnn = OUTPUT_DNN()\n",
    "\n",
    "augmentation_predictions = []\n",
    "for i in range(NB_AUGMENTATIONS):\n",
    "    group_by_key = key_generator(stacked_raw_offer_attrs)\n",
    "    mean_offer_emb, variance_offer_emb = group_by(group_by_key, offer_stacked)\n",
    "    compressed_offer_embeddings = offer_features_compressor([mean_offer_emb, variance_offer_emb])\n",
    "    mask = mask_net([mean_offer_emb, variance_offer_emb])\n",
    "    masked_offer_embeddings = apply_mask([compressed_offer_embeddings, mask])\n",
    "    _output = output_dnn(bi_linear_interaction([user_compressed, masked_offer_embeddings], generate_negatives=True))\n",
    "    augmentation_predictions.append(_output)\n",
    "output = tf.concat(augmentation_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45ded15-9fdc-457e-aeb7-64cc80958780",
   "metadata": {},
   "source": [
    "And for evaluation we don't need to create augmentations, we need just to take offer features' mean and variance coming from inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a9c050d2-9ca3-40fe-858c-2a6ba2780b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "compressed_offer_embeddings = offer_features_compressor([offer_stacked, offer_variance])\n",
    "mask = mask_net([offer_stacked, offer_variance])\n",
    "masked_offer_embeddings = apply_mask([compressed_offer_embeddings, mask])\n",
    "\n",
    "eval_output = output_dnn(bi_linear_interaction([user_compressed, masked_offer_embeddings], generate_negatives=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2cc3833",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import BroadcastLoss, BroadcastMetric\n",
    "\n",
    "model = tf.keras.Model(inputs, output, name='group_by_augmentations')\n",
    "model.compile(optimizer=OPTIMIZER,\n",
    "              loss=BroadcastLoss(LOSS, NUMBER_OF_NEGATIVES),\n",
    "              metrics=[BroadcastMetric(AUC_METRIC, NUMBER_OF_NEGATIVES)])\n",
    "\n",
    "eval_model = tf.keras.Model(inputs, eval_output, name='group_by_augmentations_eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8b3f9fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True, to_file=f'models/{DATASET}_group_by_augmentations.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e81c8b1",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ce05bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'userId'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/231 [==============================] - 180s 546ms/step - loss: 0.5173 - auc: 0.5425 - val_loss: 0.4980 - val_auc: 0.5764\n",
      "Epoch 2/10\n",
      "231/231 [==============================] - 107s 432ms/step - loss: 0.4941 - auc: 0.5949 - val_loss: 0.4954 - val_auc: 0.5931\n",
      "Epoch 3/10\n",
      "231/231 [==============================] - 106s 436ms/step - loss: 0.4911 - auc: 0.6100 - val_loss: 0.4986 - val_auc: 0.5758\n",
      "Epoch 4/10\n",
      "231/231 [==============================] - 125s 508ms/step - loss: 0.4897 - auc: 0.6150 - val_loss: 0.4908 - val_auc: 0.6164\n",
      "Epoch 5/10\n",
      "231/231 [==============================] - 129s 522ms/step - loss: 0.4886 - auc: 0.6192 - val_loss: 0.4890 - val_auc: 0.6242\n",
      "Epoch 6/10\n",
      "231/231 [==============================] - 147s 599ms/step - loss: 0.4885 - auc: 0.6190 - val_loss: 0.4940 - val_auc: 0.6018\n",
      "Epoch 7/10\n",
      "231/231 [==============================] - 139s 565ms/step - loss: 0.4883 - auc: 0.6200 - val_loss: 0.4948 - val_auc: 0.5960\n",
      "Epoch 8/10\n",
      "231/231 [==============================] - 136s 553ms/step - loss: 0.4886 - auc: 0.6181 - val_loss: 0.4895 - val_auc: 0.6172\n",
      "Epoch 9/10\n",
      "231/231 [==============================] - 139s 564ms/step - loss: 0.4876 - auc: 0.6225 - val_loss: 0.4922 - val_auc: 0.6111\n",
      "Epoch 10/10\n",
      "231/231 [==============================] - 134s 543ms/step - loss: 0.4880 - auc: 0.6203 - val_loss: 0.4895 - val_auc: 0.6212\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fea4021c100>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(datasets['train'], epochs=EPOCHS, validation_data=datasets['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27080ac6-2c87-49b0-b942-7c9bc02c29a0",
   "metadata": {},
   "source": [
    "## Single task models benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb00e26-0475-459a-9973-5aeb2fc8faf3",
   "metadata": {},
   "source": [
    "Using same approach as in [the simple model notebook](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/training/movielens%20simple%20model.ipynb) we will look at performance gap between the model with group by augmentations against set of models specialized on tasks corresponding to one offer feature at time. We won't use augmentations in those baseline models, because they will be already aligned with offer we will use in evaluation afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "373994d8-b6ec-4d7c-b878-b8c1f7bc58bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# offer columns we want to evaluate, specific to dataset we test\n",
    "TASKS = ['imdbId', 'director', 'genre']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2170ef73-1e06-4dda-9463-be8c1e71a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_linear_interaction_model(single_task_feature, name='two_tower_model'):\n",
    "    # user_features, vocabulary_sizes, EMBEDDING_DIM, REGULARIZER, USER_TOWER, OFFER_TOWER,\n",
    "    # OPTIMIZER, LOSS, NUMBER_OF_NEGATIVES\n",
    "    # come from global scope, but can be passed as params instead\n",
    "    inputs = {}\n",
    "    embedded_user_features, embedded_offer_features, variance_offer_features = {}, {}, {}\n",
    "    for feature in user_features:\n",
    "        inputs[feature] = get_input_layer(feature)\n",
    "        emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                       EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                       embeddings_regularizer=REGULARIZER())\n",
    "        embedded_user_features[feature] = emb_layer(inputs[feature])\n",
    "\n",
    "    # for offer feature we need weights:\n",
    "    # with dummy weights during training, and the ones used for a feature's averaging at inference time\n",
    "    inputs[f'{single_task_feature}_weight'] = get_input_layer(f'{single_task_feature}_weight', tf.float32)\n",
    "    inputs[single_task_feature] = get_input_layer(single_task_feature)\n",
    "    emb_layer = WeightedEmbeddings(vocabulary_sizes[single_task_feature],\n",
    "                                   EMBEDDING_DIM, name=f'{single_task_feature}_embedding',\n",
    "                                   embeddings_regularizer=REGULARIZER())\n",
    "    embedded_offer_feature = emb_layer(inputs[single_task_feature],\n",
    "                                       inputs[f'{single_task_feature}_weight'])\n",
    "    \n",
    "    user_stacked = tf.stack(list(embedded_user_features.values()), axis=1)\n",
    "    offer_stacked = tf.expand_dims(embedded_offer_feature, axis=1)\n",
    "    \n",
    "    \n",
    "    user_compressed = UserFeaturesCompressor(USER_META_FEATURES, DROPOUT,\n",
    "                                             name='user_compressor')(user_stacked)\n",
    "    mask_net = MaskNet(OFFER_META_FEATURES, DROPOUT, name='mask_generation')\n",
    "    apply_mask = tf.keras.layers.Multiply(name='apply_mask')\n",
    "    bi_linear_interaction = BiLinearInteraction(number_of_negatives=NUMBER_OF_NEGATIVES, dropout_rate=DROPOUT,\n",
    "                                                initializer='random_normal', regularizer=REGULARIZER(),\n",
    "                                                name='interaction')\n",
    "    output_dnn = OUTPUT_DNN()\n",
    "\n",
    "    \n",
    "    mask = mask_net([offer_stacked, offer_stacked])\n",
    "    masked_offer_embeddings = apply_mask([offer_stacked, mask])\n",
    "    \n",
    "    output = OUTPUT_DNN()(bi_linear_interaction([user_compressed, masked_offer_embeddings],\n",
    "                                                generate_negatives=True))\n",
    "\n",
    "    model = tf.keras.Model(inputs, output, name=name)\n",
    "    model.compile(optimizer=OPTIMIZER,\n",
    "                  loss=BroadcastLoss(LOSS, NUMBER_OF_NEGATIVES),\n",
    "                  metrics=[BroadcastMetric(AUC_METRIC, NUMBER_OF_NEGATIVES)])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "34c6b37f-b971-41b6-bc36-e60974bcdb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer RandomNormal is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'genre', 'titleType', 'actor', 'userId', 'startYearCluster', 'runtimeMinutesCluster', 'director', 'genre_weight', 'titleType_weight', 'actor_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight', 'director_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/231 [==============================] - 145s 465ms/step - loss: 0.5037 - auc: 0.5882 - val_loss: 0.5042 - val_auc: 0.5941\n",
      "Epoch 2/10\n",
      "231/231 [==============================] - 104s 423ms/step - loss: 0.4887 - auc: 0.6360 - val_loss: 0.4915 - val_auc: 0.6284\n",
      "Epoch 3/10\n",
      "231/231 [==============================] - 110s 444ms/step - loss: 0.4835 - auc: 0.6523 - val_loss: 0.4896 - val_auc: 0.6353\n",
      "Epoch 4/10\n",
      "231/231 [==============================] - 85s 335ms/step - loss: 0.4804 - auc: 0.6615 - val_loss: 0.4879 - val_auc: 0.6421\n",
      "Epoch 5/10\n",
      "231/231 [==============================] - 94s 369ms/step - loss: 0.4782 - auc: 0.6678 - val_loss: 0.4867 - val_auc: 0.6459\n",
      "Epoch 6/10\n",
      "231/231 [==============================] - 101s 400ms/step - loss: 0.4768 - auc: 0.6709 - val_loss: 0.4861 - val_auc: 0.6475\n",
      "Epoch 7/10\n",
      "231/231 [==============================] - 101s 405ms/step - loss: 0.4758 - auc: 0.6733 - val_loss: 0.4856 - val_auc: 0.6471\n",
      "Epoch 8/10\n",
      "231/231 [==============================] - 101s 407ms/step - loss: 0.4751 - auc: 0.6747 - val_loss: 0.4852 - val_auc: 0.6494\n",
      "Epoch 9/10\n",
      "231/231 [==============================] - 92s 367ms/step - loss: 0.4744 - auc: 0.6762 - val_loss: 0.4854 - val_auc: 0.6496\n",
      "Epoch 10/10\n",
      "231/231 [==============================] - 102s 405ms/step - loss: 0.4740 - auc: 0.6773 - val_loss: 0.4852 - val_auc: 0.6505\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'genre', 'titleType', 'actor', 'userId', 'imdbId', 'startYearCluster', 'runtimeMinutesCluster', 'genre_weight', 'titleType_weight', 'actor_weight', 'imdbId_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/231 [==============================] - 134s 439ms/step - loss: 0.5014 - auc: 0.5817 - val_loss: 0.5016 - val_auc: 0.5924\n",
      "Epoch 2/10\n",
      "231/231 [==============================] - 117s 467ms/step - loss: 0.4904 - auc: 0.6166 - val_loss: 0.4929 - val_auc: 0.6077\n",
      "Epoch 3/10\n",
      "231/231 [==============================] - 124s 499ms/step - loss: 0.4873 - auc: 0.6279 - val_loss: 0.4913 - val_auc: 0.6151\n",
      "Epoch 4/10\n",
      "231/231 [==============================] - 102s 401ms/step - loss: 0.4855 - auc: 0.6341 - val_loss: 0.4900 - val_auc: 0.6193\n",
      "Epoch 5/10\n",
      "231/231 [==============================] - 114s 457ms/step - loss: 0.4842 - auc: 0.6379 - val_loss: 0.4898 - val_auc: 0.6204\n",
      "Epoch 6/10\n",
      "231/231 [==============================] - 100s 402ms/step - loss: 0.4835 - auc: 0.6398 - val_loss: 0.4896 - val_auc: 0.6215\n",
      "Epoch 7/10\n",
      "231/231 [==============================] - 102s 409ms/step - loss: 0.4828 - auc: 0.6414 - val_loss: 0.4888 - val_auc: 0.6222\n",
      "Epoch 8/10\n",
      "231/231 [==============================] - 99s 396ms/step - loss: 0.4824 - auc: 0.6426 - val_loss: 0.4889 - val_auc: 0.6231\n",
      "Epoch 9/10\n",
      "231/231 [==============================] - 100s 398ms/step - loss: 0.4820 - auc: 0.6433 - val_loss: 0.4888 - val_auc: 0.6228\n",
      "Epoch 10/10\n",
      "231/231 [==============================] - 97s 385ms/step - loss: 0.4817 - auc: 0.6446 - val_loss: 0.4893 - val_auc: 0.6234\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'titleType', 'actor', 'userId', 'imdbId', 'startYearCluster', 'runtimeMinutesCluster', 'director', 'titleType_weight', 'actor_weight', 'imdbId_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight', 'director_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231/231 [==============================] - 138s 453ms/step - loss: 0.5054 - auc: 0.5305 - val_loss: 0.5070 - val_auc: 0.5150\n",
      "Epoch 2/10\n",
      "231/231 [==============================] - 90s 355ms/step - loss: 0.4996 - auc: 0.5411 - val_loss: 0.5001 - val_auc: 0.5359\n",
      "Epoch 3/10\n",
      "231/231 [==============================] - 103s 409ms/step - loss: 0.4987 - auc: 0.5487 - val_loss: 0.4991 - val_auc: 0.5462\n",
      "Epoch 4/10\n",
      "231/231 [==============================] - 89s 349ms/step - loss: 0.4983 - auc: 0.5516 - val_loss: 0.4989 - val_auc: 0.5479\n",
      "Epoch 5/10\n",
      "231/231 [==============================] - 88s 350ms/step - loss: 0.4981 - auc: 0.5532 - val_loss: 0.4988 - val_auc: 0.5483\n",
      "Epoch 6/10\n",
      "231/231 [==============================] - 97s 384ms/step - loss: 0.4979 - auc: 0.5546 - val_loss: 0.4987 - val_auc: 0.5489\n",
      "Epoch 7/10\n",
      "231/231 [==============================] - 97s 384ms/step - loss: 0.4978 - auc: 0.5549 - val_loss: 0.4988 - val_auc: 0.5491\n",
      "Epoch 8/10\n",
      "231/231 [==============================] - 90s 362ms/step - loss: 0.4977 - auc: 0.5557 - val_loss: 0.4987 - val_auc: 0.5489\n",
      "Epoch 9/10\n",
      "231/231 [==============================] - 97s 386ms/step - loss: 0.4976 - auc: 0.5563 - val_loss: 0.4986 - val_auc: 0.5490\n",
      "Epoch 10/10\n",
      "231/231 [==============================] - 97s 386ms/step - loss: 0.4975 - auc: 0.5568 - val_loss: 0.4988 - val_auc: 0.5497\n"
     ]
    }
   ],
   "source": [
    "mono_feature_models = {}\n",
    "for task_offer_feature in TASKS:\n",
    "    mono_feature_models[task_offer_feature] =\\\n",
    "        bi_linear_interaction_model(task_offer_feature, name=f'{task_offer_feature}_model')\n",
    "    mono_feature_models[task_offer_feature].fit(datasets['train'],\n",
    "                                                epochs=EPOCHS,\n",
    "                                                validation_data=datasets['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49cc7d11-391a-4466-9bb5-bf9f2bbcc203",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "eb861dcc-1c7c-4a3e-be52-864c97543ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 3s, sys: 11 s, total: 3min 14s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from utils import prepare_single_task_dataset\n",
    "test_datasets = {}\n",
    "for task_offer_feature in TASKS:\n",
    "    test_datasets[task_offer_feature] = \\\n",
    "        prepare_single_task_dataset(datasets['test'], task_offer_feature, offer_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "644699ac-98a0-496d-8a76-afb73852bc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'genre', 'titleType', 'actor', 'userId', 'imdbId', 'startYearCluster', 'runtimeMinutesCluster', 'genre_weight', 'titleType_weight', 'actor_weight', 'imdbId_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'titleType', 'actor', 'userId', 'imdbId', 'startYearCluster', 'runtimeMinutesCluster', 'director', 'titleType_weight', 'actor_weight', 'imdbId_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight', 'director_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'userId'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'genre', 'titleType', 'actor', 'userId', 'startYearCluster', 'runtimeMinutesCluster', 'director', 'genre_weight', 'titleType_weight', 'actor_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight', 'director_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'genre', 'titleType', 'actor', 'userId', 'imdbId', 'startYearCluster', 'runtimeMinutesCluster', 'genre_weight', 'titleType_weight', 'actor_weight', 'imdbId_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'titleType', 'actor', 'userId', 'imdbId', 'startYearCluster', 'runtimeMinutesCluster', 'director', 'titleType_weight', 'actor_weight', 'imdbId_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight', 'director_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'userId'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'genre', 'titleType', 'actor', 'userId', 'startYearCluster', 'runtimeMinutesCluster', 'director', 'genre_weight', 'titleType_weight', 'actor_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight', 'director_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'genre', 'titleType', 'actor', 'userId', 'imdbId', 'startYearCluster', 'runtimeMinutesCluster', 'genre_weight', 'titleType_weight', 'actor_weight', 'imdbId_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'titleType', 'actor', 'userId', 'imdbId', 'startYearCluster', 'runtimeMinutesCluster', 'director', 'titleType_weight', 'actor_weight', 'imdbId_weight', 'startYearCluster_weight', 'runtimeMinutesCluster_weight', 'director_weight'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n",
      "/usr/local/lib/python3.9/site-packages/keras/engine/functional.py:638: UserWarning: Input dict contained keys ['date', 'userId'] which did not match any model input. They will be ignored by the model.\n",
      "  inputs = self._flatten_to_reference_inputs(inputs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 59s, sys: 1min 50s, total: 16min 50s\n",
      "Wall time: 5min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "from utils import evaluate_model, wAUC\n",
    "\n",
    "aucs = defaultdict(dict)\n",
    "for task_offer_feature in TASKS:\n",
    "    for model_name in TASKS:\n",
    "        aucs[task_offer_feature][f'MONO:{model_name}'] = \\\n",
    "            evaluate_model(mono_feature_models[model_name],\n",
    "                           task_offer_feature, test_datasets, NUMBER_OF_NEGATIVES, inverse_lookups)\n",
    "    aucs[task_offer_feature]['group_by augmentations'] = \\\n",
    "            evaluate_model(eval_model, task_offer_feature, test_datasets, NUMBER_OF_NEGATIVES, inverse_lookups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f862e7c9-c599-408c-8572-d2f924a36806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "results = pd.DataFrame()\n",
    "for task_name in aucs:\n",
    "    for model_name in aucs[task_name]:\n",
    "        w_auc = wAUC(aucs[task_name][model_name])\n",
    "        results = pd.concat([results,\n",
    "                             pd.Series({'wAUC': w_auc, 'offers': task_name, 'model': model_name}).to_frame().T],\n",
    "                            ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec9ac730-c8f3-4a95-8102-a2551742f98f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a3be3_row0_col0 {\n",
       "  background-color: #b50927;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3be3_row0_col1 {\n",
       "  background-color: #4a63d3;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3be3_row0_col2 {\n",
       "  background-color: #ef886b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3be3_row1_col0, #T_a3be3_row1_col2, #T_a3be3_row2_col1 {\n",
       "  background-color: #3b4cc0;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3be3_row1_col1, #T_a3be3_row2_col2, #T_a3be3_row3_col0 {\n",
       "  background-color: #b40426;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3be3_row2_col0 {\n",
       "  background-color: #c32e31;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_a3be3_row3_col1 {\n",
       "  background-color: #f7b396;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_a3be3_row3_col2 {\n",
       "  background-color: #b8122a;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a3be3\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >offers</th>\n",
       "      <th id=\"T_a3be3_level0_col0\" class=\"col_heading level0 col0\" >director</th>\n",
       "      <th id=\"T_a3be3_level0_col1\" class=\"col_heading level0 col1\" >genre</th>\n",
       "      <th id=\"T_a3be3_level0_col2\" class=\"col_heading level0 col2\" >film</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >model</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a3be3_level0_row0\" class=\"row_heading level0 row0\" >MONO:director</th>\n",
       "      <td id=\"T_a3be3_row0_col0\" class=\"data row0 col0\" >0.594</td>\n",
       "      <td id=\"T_a3be3_row0_col1\" class=\"data row0 col1\" >0.538</td>\n",
       "      <td id=\"T_a3be3_row0_col2\" class=\"data row0 col2\" >0.593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3be3_level0_row1\" class=\"row_heading level0 row1\" >MONO:genre</th>\n",
       "      <td id=\"T_a3be3_row1_col0\" class=\"data row1 col0\" >0.535</td>\n",
       "      <td id=\"T_a3be3_row1_col1\" class=\"data row1 col1\" >0.558</td>\n",
       "      <td id=\"T_a3be3_row1_col2\" class=\"data row1 col2\" >0.527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3be3_level0_row2\" class=\"row_heading level0 row2\" >MONO:film</th>\n",
       "      <td id=\"T_a3be3_row2_col0\" class=\"data row2 col0\" >0.592</td>\n",
       "      <td id=\"T_a3be3_row2_col1\" class=\"data row2 col1\" >0.537</td>\n",
       "      <td id=\"T_a3be3_row2_col2\" class=\"data row2 col2\" >0.611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a3be3_level0_row3\" class=\"row_heading level0 row3\" >group_by augmentations</th>\n",
       "      <td id=\"T_a3be3_row3_col0\" class=\"data row3 col0\" >0.594</td>\n",
       "      <td id=\"T_a3be3_row3_col1\" class=\"data row3 col1\" >0.551</td>\n",
       "      <td id=\"T_a3be3_row3_col2\" class=\"data row3 col2\" >0.610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7feb44236460>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.pivot_table(results, 'wAUC', 'model', 'offers')\\\n",
    "    .rename(columns={'imdbId': 'film'}, index={'MONO:imdbId': 'MONO:film'})\\\n",
    "    .style.background_gradient(cmap='coolwarm').format(precision=3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [moksha-tf2-cpu.2-7] (Local)",
   "language": "python",
   "name": "local-eu.gcr.io_tinyclues-experiments_tinyclues_moksha-tf2-cpu.2-7_latest__moksha"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
