{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55b9dd37-bc41-46af-9aa1-e20ee99ecdd0",
   "metadata": {},
   "source": [
    "# Preparing MovieLens+IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2990553d-ecf8-4dd8-b5e3-f49b57b62d64",
   "metadata": {},
   "source": [
    "## Downloading raw datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd2ee2b-4ecf-4d4e-974c-7014405e5e21",
   "metadata": {},
   "source": [
    "Datasets descriptions are available at\n",
    "* http://files.grouplens.org/datasets/movielens/ml-20m-README.html for MovieLens-20M\n",
    "* https://www.imdb.com/interfaces/ for IMDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39b06624-84f7-4d40-b626-3709badc75ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 13:38:53.877596: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2022-10-18 13:38:53.877644: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/jupyter/recsys-multi-atrribute-benchmark/datasets'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import DATASETS_ROOT_DIR\n",
    "# Directory where those datasets will be downloaded:\n",
    "DATASETS_ROOT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4173427-9c6e-43ea-a1fa-5f715f0c0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://files.grouplens.org/datasets/movielens/ml-20m.zip -P $DATASETS_ROOT_DIR/raw\n",
    "!unzip $DATASETS_ROOT_DIR/raw/ml-20m.zip -d $DATASETS_ROOT_DIR/raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d216e12-2e5a-462c-adc1-b41306a1f3b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downloading only some IMDB files needed for our experiment\n",
    "for file_name in [\n",
    "    'name.basics.tsv.gz',\n",
    "    # 'title.akas.tsv.gz',\n",
    "    'title.basics.tsv.gz',\n",
    "    'title.crew.tsv.gz',\n",
    "    # 'title.episode.tsv.gz',\n",
    "    'title.principals.tsv.gz',\n",
    "    # 'title.ratings.tsv.gz',\n",
    "]:\n",
    "    !wget https://datasets.imdbws.com/$file_name -P $DATASETS_ROOT_DIR/raw/imdb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da3d9cd-075c-485b-a8c3-a9923acaef64",
   "metadata": {},
   "source": [
    "## Choosing features from IMDB dataset & joining datasets together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f52dc839-2a41-4cae-b3a9-7866ec690769",
   "metadata": {},
   "source": [
    "Here we load IMDB dataset and keep some attributes we will use further as film features:\n",
    "* `primaryTitle` (name of a film)\n",
    "* `genres` (list of up to 3 genres associated with films)\n",
    "* `actors` (list of actors of the given film)\n",
    "* `directors` (list of directors of the given film)\n",
    "* `titleType` (e.g. movie, short, tvseries, tvepisode, video, etc)\n",
    "* `startYear` (year of a film's release / initial release of TV series)\n",
    "* `runtimeMinutes` (film's duration in minutes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fd60f67-dc00-4413-ab84-a7c05da006f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/IPython/core/magics/execution.py:1335: DtypeWarning: Columns (4) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  exec(code, glob, local_ns)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 6s, sys: 10.6 s, total: 3min 16s\n",
      "Wall time: 3min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import os\n",
    "import gzip\n",
    "import pandas as pd\n",
    "\n",
    "with gzip.open(os.path.join(DATASETS_ROOT_DIR, 'raw/imdb/title.basics.tsv.gz'), 'rb') as f:\n",
    "    imdb_titles = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "with gzip.open(os.path.join(DATASETS_ROOT_DIR, 'raw/imdb/title.principals.tsv.gz'), 'rb') as f:\n",
    "    imdb_principals = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "with gzip.open(os.path.join(DATASETS_ROOT_DIR, 'raw/imdb/title.crew.tsv.gz'), 'rb') as f:\n",
    "    imdb_crew = pd.read_csv(f, sep='\\t')\n",
    "\n",
    "# for each dataframe we set film id cast to the integer as an index\n",
    "for dataframe in [imdb_titles, imdb_principals, imdb_crew]:\n",
    "    dataframe['tconst'] = dataframe['tconst'].map(lambda x: int(x[2:]))\n",
    "    dataframe.set_index('tconst', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c97364-70f0-42ef-ab8c-b1e47170fbe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 4s, sys: 1.21 s, total: 1min 5s\n",
      "Wall time: 1min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# getting list of actors for each film\n",
    "imdb_titles['actors'] = imdb_principals[np.isin(imdb_principals.category, ['actor', 'actress'])]\\\n",
    "    .groupby('tconst')['nconst'].agg(','.join)\n",
    "\n",
    "# getting list of directors for each film\n",
    "imdb_titles['directors'] = imdb_crew['directors']\n",
    "\n",
    "del imdb_principals\n",
    "del imdb_crew"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d8003ac5-258e-476e-87ae-851b623d75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_links = pd.read_csv(os.path.join(DATASETS_ROOT_DIR, 'raw/ml-20m/links.csv'))\n",
    "ml_ratings = pd.read_csv(os.path.join(DATASETS_ROOT_DIR, 'raw/ml-20m/ratings.csv'))\n",
    "\n",
    "ml_ratings = pd.merge(ml_ratings, ml_links[['movieId', 'imdbId']], on='movieId', how='left')\n",
    "\n",
    "del ml_links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bbd7f918-c9ee-4a63-9fa0-fb34ba1fc1a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining MovieLens dataset with IMDB attributes\n",
    "imdb_features = ['titleType', 'primaryTitle', 'startYear', 'runtimeMinutes', 'genres', 'actors', 'directors']\n",
    "ratings = pd.merge(ml_ratings, imdb_titles[imdb_features],\n",
    "                   how='left', left_on='imdbId', right_index=True)\n",
    "\n",
    "del imdb_titles\n",
    "del ml_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56960815-584f-4054-a3d2-7842ce3536e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null values count\n",
      "actors            236605\n",
      "directors          40687\n",
      "genres             19400\n",
      "startYear          18190\n",
      "runtimeMinutes     18969\n",
      "primaryTitle       18190\n",
      "dtype: int64\n",
      "from 20000263\n",
      "Replacing null startYear by 1995\n",
      "Replacing null runtimeMinutes by 112\n"
     ]
    }
   ],
   "source": [
    "# Correcting dtype\n",
    "ratings['date'] = ratings['timestamp'].astype('datetime64[s]').astype('datetime64[D]')\n",
    "\n",
    "# Set correct null values\n",
    "ratings['directors'] = ratings['directors'].replace('\\\\N', np.nan)\n",
    "ratings['genres'] = ratings['genres'].replace('\\\\N', np.nan)\n",
    "ratings['runtimeMinutes'] = ratings['runtimeMinutes'].replace('\\\\N', np.nan).astype(float)\n",
    "\n",
    "# Null stats\n",
    "print('Null values count')\n",
    "print(ratings[['actors', 'directors', 'genres', 'startYear', 'runtimeMinutes', 'primaryTitle']].isnull().sum())\n",
    "print(f'from {len(ratings)}')\n",
    "\n",
    "# Choose median values to replace nulls\n",
    "median_year = int(ratings['startYear'].dropna().astype(int).median())\n",
    "median_runtime = int(ratings['runtimeMinutes'].dropna().median())\n",
    "print(f'Replacing null startYear by {median_year}')\n",
    "print(f'Replacing null runtimeMinutes by {median_runtime}')\n",
    "\n",
    "# Filling null values\n",
    "ratings['actors'] = ratings['actors'].fillna('')\n",
    "ratings['directors'] = ratings['directors'].replace('\\\\N', '').fillna('')\n",
    "ratings['genres'] = ratings['genres'].replace('\\\\N', '').fillna('')\n",
    "ratings['startYear'] = ratings['startYear'].fillna(median_year).astype(int)\n",
    "ratings['runtimeMinutes'] = ratings['runtimeMinutes'].fillna(median_runtime).astype(int)\n",
    "ratings['primaryTitle'] = ratings['primaryTitle'].fillna('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ca8e5d-1a9c-40ff-941f-b8118a3ec2df",
   "metadata": {},
   "source": [
    "Some films have same titles so, we will use `imdbId` as a feature, and not `primaryTitle`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35d0aa1-b214-4295-a378-70ad73c6ffc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "primaryTitle\n",
       "                                69\n",
       "12 Angry Men                     2\n",
       "13                               2\n",
       "1984                             2\n",
       "20,000 Leagues Under the Sea     4\n",
       "Name: imdbId, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "films_by_title = ratings.groupby('primaryTitle')['imdbId'].agg(lambda x: len(set(x)))\n",
    "films_by_title[films_by_title != 1].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217dd24e-c620-44c6-839e-090be49194f2",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5e525-d04f-42b7-bdd9-48a086158d9a",
   "metadata": {},
   "source": [
    "Here we will do some simple feature engineering on IMDB attributes:\n",
    "* clusterize film's release year\n",
    "* clusterize film's duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ad2b252-6d42-4cf3-bc52-92c12c1c52be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans_runtime = KMeans(n_clusters=25, random_state=42)\n",
    "kmeans_runtime.fit(ratings['runtimeMinutes'].sample(5 * 10 ** 5, random_state=43).values.reshape(-1, 1))\n",
    "cluster_labels = np.round(kmeans_runtime.cluster_centers_[:, 0], 0).astype(int)\n",
    "assert len(np.unique(cluster_labels)) == 25\n",
    "ratings['runtimeMinutesCluster'] = cluster_labels[kmeans_runtime.predict(ratings['runtimeMinutes'].values.reshape(-1, 1))]\n",
    "\n",
    "kmeans_year = KMeans(n_clusters=30, random_state=42)\n",
    "kmeans_year.fit(ratings['startYear'].sample(5 * 10 ** 5, random_state=43).values.reshape(-1, 1))\n",
    "cluster_labels = np.round(kmeans_year.cluster_centers_[:, 0], 0).astype(int)\n",
    "assert len(np.unique(cluster_labels)) == 30\n",
    "ratings['startYearCluster'] = cluster_labels[kmeans_year.predict(ratings['startYear'].values.reshape(-1, 1))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bde217d-0cec-4b8e-8485-9ce5bf7d7049",
   "metadata": {},
   "source": [
    "Also for further simplicity of the script we will\n",
    "* keep random genre tag\n",
    "* keep most popular director instead of list of directors\n",
    "* also keep most popular actor's name for each film\n",
    "\n",
    "\n",
    "That will allow us to work only with categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e924a90-7d22-4b5f-81db-4d714af2548f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "unique_films = ratings.drop_duplicates(['imdbId'])\n",
    "film_popularity = ratings['imdbId'].value_counts().to_dict()\n",
    "\n",
    "actor_popularity, director_popularity = defaultdict(int), defaultdict(int)\n",
    "for film, actors, directors in zip(unique_films['imdbId'], unique_films['actors'], unique_films['directors']):\n",
    "    for actor in actors.split(','):\n",
    "        actor_popularity[actor] += film_popularity[film]\n",
    "    for director in directors.split(','):\n",
    "        director_popularity[director] += film_popularity[film]\n",
    "        \n",
    "popular_actors, popular_directors, random_genres = [], [], []\n",
    "np.random.seed(1729)\n",
    "for actors, directors, genres in zip(unique_films['actors'], unique_films['directors'], unique_films['genres']):\n",
    "    popular_actors.append(max(actors.split(','), key=actor_popularity.__getitem__))\n",
    "    popular_directors.append(max(directors.split(','), key=director_popularity.__getitem__))\n",
    "    random_genres.append(np.random.choice(genres.split(','), size=1)[0])\n",
    "np.random.seed(None)\n",
    "\n",
    "unique_films['actor_id'] = popular_actors\n",
    "unique_films['director_id'] = popular_directors\n",
    "unique_films['genre'] = random_genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb9e050b-63a7-4e43-88c4-528573175a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace actors and directors by their name for more readability\n",
    "with gzip.open(os.path.join(DATASETS_ROOT_DIR, 'raw/imdb/name.basics.tsv.gz'), 'rb') as f:\n",
    "    imdb_names = pd.read_csv(f, sep='\\t')\n",
    "imdb_names.set_index('nconst', inplace=True)\n",
    "imdb_names['actor'] = imdb_names['director'] = imdb_names['primaryName']\n",
    "\n",
    "unique_films = pd.merge(unique_films, imdb_names[['actor']],\n",
    "         how='left', left_on='actor_id', right_index=True)\n",
    "unique_films = pd.merge(unique_films, imdb_names[['director']],\n",
    "         how='left', left_on='director_id', right_index=True)\n",
    "\n",
    "unique_films.loc[unique_films['director'].isnull(), 'director'] =\\\n",
    "    unique_films.loc[unique_films['director'].isnull(), 'director_id']\n",
    "\n",
    "# merging everything back to ratings table\n",
    "ratings = pd.merge(ratings, unique_films[['genre', 'actor', 'director', 'imdbId']],\n",
    "                   how='left', left_on='imdbId', right_on='imdbId')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda1d3de-a1f6-4249-be82-cf471222e3da",
   "metadata": {},
   "source": [
    "## Keep only rating >= 4\n",
    "To consider only implicit interactions we keep only explicit ratings of four or higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fcf73359-259a-4558-9672-dffb42225fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keep 9995410 lines from 20000263\n"
     ]
    }
   ],
   "source": [
    "implicit_ratings = ratings[ratings['rating'] >= 4.]\n",
    "print(f'Keep {len(implicit_ratings)} lines from {len(ratings)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2121876e-6bc5-4351-9ca9-c067d411a7c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# getting train/val/test split same as in https://github.com/zombak79/vasp/blob/main/MovieLens%20-%20preprocessing.ipynb\n",
    "implicit_ratings['nb_ratings'] = implicit_ratings.groupby('userId')['imdbId'].transform('count')\n",
    "_users = implicit_ratings[implicit_ratings['nb_ratings'] >= 5].sort_values(['userId','timestamp'])['userId'].drop_duplicates()\n",
    "_users.index = _users.index.astype(str)\n",
    "_users = _users.sort_index().sample(frac=1., random_state=42)\n",
    "test_users = _users.iloc[:10000].values\n",
    "val_users = _users.iloc[10000:20000].values\n",
    "del _users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ae5808-5a9c-4c1b-8ff6-32df28975f7a",
   "metadata": {},
   "source": [
    "## Encoding features and converting them into `tf.Tensor`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aba699-7bfd-4c7c-bfab-1a59b170a987",
   "metadata": {},
   "source": [
    "Now let's encode categorical features into ordinal using `tf.keras.layers.StringLookup` and transform out dataset into a dictionary with `tf.Tensor` for each column - result will be kept in `tf_tensors`.\n",
    "\n",
    "We also keep track of inverse transformation should we want to see a value corresponding to some label - it will be stored in `inverse_lookups` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f083889-1a08-4242-9080-7695fa7922ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-18 13:47:34.202686: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/cuda/lib:/usr/local/lib/x86_64-linux-gnu:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:\n",
      "2022-10-18 13:47:34.248764: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-10-18 13:47:34.248892: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (26869bb83a96): /proc/driver/nvidia/version does not exist\n",
      "2022-10-18 13:47:34.331441: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoding imdbId column\n",
      "Reserving labels for 7884 categories out of 20720\n",
      "Encoding titleType column\n",
      "Reserving labels for 10 categories out of 10\n",
      "Encoding genre column\n",
      "Reserving labels for 26 categories out of 27\n",
      "Encoding actor column\n",
      "Reserving labels for 2497 categories out of 6597\n",
      "Encoding director column\n",
      "Reserving labels for 3094 categories out of 8166\n",
      "Encoding runtimeMinutesCluster column\n",
      "Reserving labels for 25 categories out of 25\n",
      "Encoding startYearCluster column\n",
      "Reserving labels for 30 categories out of 30\n",
      "CPU times: user 55.5 s, sys: 2.93 s, total: 58.4 s\n",
      "Wall time: 58.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from utils import get_tensorflow_dataset\n",
    "\n",
    "item_features = ['imdbId', 'titleType', 'genre', 'actor', 'director',\n",
    "                 'runtimeMinutesCluster', 'startYearCluster']\n",
    "\n",
    "tf_tensors, inverse_lookups = get_tensorflow_dataset(implicit_ratings, item_features,\n",
    "                                                     user_id_column='userId', date_column='date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd792d9-7680-4308-a0b5-7e4b11b86d3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'userId': <tf.Tensor: shape=(2,), dtype=int64, numpy=array([1, 1])>,\n",
       " 'date': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([12671, 12875], dtype=int32)>,\n",
       " 'imdbId': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([355,  94], dtype=int32)>,\n",
       " 'titleType': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([10, 10], dtype=int32)>,\n",
       " 'genre': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([20, 11], dtype=int32)>,\n",
       " 'actor': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 31, 140], dtype=int32)>,\n",
       " 'director': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([258,  55], dtype=int32)>,\n",
       " 'runtimeMinutesCluster': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([20, 16], dtype=int32)>,\n",
       " 'startYearCluster': <tf.Tensor: shape=(2,), dtype=int32, numpy=array([12, 15], dtype=int32)>}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils import gather_structure\n",
    "gather_structure(tf_tensors, [0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e80888b1-2649-4a01-a365-79933c636a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.preprocessing.string_lookup.StringLookup at 0x7fad00699310>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_lookups['actor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae48ff00-9c48-4eb3-af30-834d4aeeb197",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'Liam Neeson', b'Jeff Anderson'], dtype=object)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_lookups['actor'](tf_tensors['actor'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b01b0f0f-1fb9-436b-b2f8-8760bd5be9b1",
   "metadata": {},
   "source": [
    "For film ids we can replace reverse mapping by film names for more readability if needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f012fce1-9150-4190-88fd-a03971c7ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from utils import enforce_unique_values\n",
    "\n",
    "film_names = enforce_unique_values(implicit_ratings.groupby('imdbId')['primaryTitle'].first().to_dict())\n",
    "\n",
    "new_vocab = list(map(lambda x: film_names[int(x)] if x != '[UNK]' else x,\n",
    "                     inverse_lookups['imdbId'].get_vocabulary()))\n",
    "inverse_lookups['imdbId'] = tf.keras.layers.StringLookup(vocabulary=new_vocab,\n",
    "                                                         invert=True,\n",
    "                                                         name=inverse_lookups['imdbId'].name,\n",
    "                                                         num_oov_indices=inverse_lookups['imdbId'].num_oov_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c179741-dc63-4f21-aba2-e9d6534b3a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'Rob Roy', b'Clerks'], dtype=object)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_lookups['imdbId'](tf_tensors['imdbId'][:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "25400d5e-2c37-46ae-b600-b2b55b98d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import save_inverse_lookups\n",
    "\n",
    "save_inverse_lookups(inverse_lookups, os.path.join(DATASETS_ROOT_DIR, 'movielens_imdb/inverse_lookups.pickle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "88addb59-9ace-403f-8e00-afa8e2e42df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "del inverse_lookups"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbe318a-dca3-42a6-a817-1a0c7988c79d",
   "metadata": {},
   "source": [
    "## Changing format to event sequences by user"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa8cf793-6253-4873-a533-ee697d0722a5",
   "metadata": {},
   "source": [
    "From this point we won't need `pd.DataFrame` anymore and will work witrh tensorflow objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23b9dfe1-10c8-46ba-9e9d-a9da95407e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "del implicit_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1e1fbc-04dd-4f8a-a62e-cb2453b2f431",
   "metadata": {},
   "source": [
    "First we apply train/val/test split defined above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63685c53-af75-4f2f-8a77-476ce6bf59d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_mask = np.isin(tf_tensors['userId'], val_users)\n",
    "test_mask = np.isin(tf_tensors['userId'], test_users)\n",
    "train_mask = (~val_mask) & (~test_mask)\n",
    "\n",
    "tensors = {}\n",
    "\n",
    "tensors['train'] = gather_structure(tf_tensors, np.where(train_mask)[0])\n",
    "tensors['val'] = gather_structure(tf_tensors, np.where(val_mask)[0])\n",
    "tensors['test'] = gather_structure(tf_tensors, np.where(test_mask)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f505999-ee5d-4fa4-a804-a26353322845",
   "metadata": {},
   "source": [
    "Then, let's define for each user and for each type of event (here we have only ratings) a sequence of corresponding events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62a2dd90-d0b4-4a41-9205-a928842ce90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_user_sequences\n",
    "\n",
    "for split_name, tensors_dict in tensors.items():\n",
    "    tensors[split_name] = get_user_sequences({'ratings': tensors_dict}, 'ratings', 'userId')\n",
    "\n",
    "del tensors_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af049ffe-af65-421e-a814-752999c076a1",
   "metadata": {},
   "source": [
    "Now our dict contains additional technical key `_user_index` that encodes what events correspond to what user. To get sequences of events one can simply gather a feature's values using this index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da1303b8-8dc5-4773-907c-30d4593bb69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.ops.ragged.ragged_tensor.RaggedTensor"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tensors['train']['ratings']['_user_index'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb1bb00-3981-441f-9dec-2a3281d360a2",
   "metadata": {},
   "source": [
    "First dimension of this tensor corresponds to unique users, second corresponds to line numbers of events for a given user. By taking bounding shape we see number of unique users and maximal number of ratings done by one user in train dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25f6a235-eeec-4f93-8b24-3a8be72f6c91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([118287,   3177], dtype=int32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors['train']['ratings']['_user_index'].bounding_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb3a9f8-76d8-4ded-bb08-7ca635fe0dae",
   "metadata": {},
   "source": [
    "Each split has its own independent index, with local indices (this property will be kept for each batch) starting from 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6f6ceef1-424d-4c44-9a8b-f2e9d25d0f6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[0, 1, 2, 3, 4],\n",
       " [88, 89, 90, 91, 92],\n",
       " [131, 132, 133, 134, 135]]>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors['train']['ratings']['_user_index'][:3, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc36c4a0-5c08-4723-a341-f230b22d54d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([10000,  1738], dtype=int32)>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors['test']['ratings']['_user_index'].bounding_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "fdff875f-8b20-4e50-a544-5d78447aa194",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[0, 1, 2, 3, 4],\n",
       " [348, 349, 350, 351, 352],\n",
       " [383, 384, 385, 386, 387]]>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensors['test']['ratings']['_user_index'][:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f934da7-3e77-42df-9ca8-f6e2e054bd74",
   "metadata": {},
   "source": [
    "To get all films seen by user it is enough to gather on correspondant tensor. In this example we limit to 3 users, 5 films:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2b940e23-d6cb-49b3-8cea-3f69b6e7493b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[355, 94, 138, 14, 67],\n",
       " [612, 170, 463, 18, 14],\n",
       " [26, 32, 17, 973, 94]]>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.gather(tensors['train']['ratings']['imdbId'], tensors['train']['ratings']['_user_index'][:3, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6167db3-140e-456b-814b-2c35d271e0f9",
   "metadata": {},
   "source": [
    "## Batching by users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3706ebdc-fefd-4f63-9253-3b5531eb6b1a",
   "metadata": {},
   "source": [
    "For further operation let's transform dicts into `tf.data.Dataset` batched by users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "632cb3c9-1d3b-4558-8688-245452732dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import batch_by_user\n",
    "\n",
    "datasets = {}\n",
    "for split_name, tensors_dict_by_event in tensors.items():\n",
    "    datasets[split_name] = batch_by_user(tensors_dict_by_event, 'ratings', 5 * 10 ** 3, seed=12345)\n",
    "    \n",
    "del tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a348ce44-72bc-4a42-acdc-4911e6648eb8",
   "metadata": {},
   "source": [
    "Now we have `tf.data.Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f484051-bb4d-4bda-b39a-ddca76c247f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.ConcatenateDataset"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(datasets['train'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9553c6-17b5-4bad-8a6d-ae1bb1c55170",
   "metadata": {},
   "source": [
    "where each batch contain 5000 unique users and batch values is a dict with event type as key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "903d5bc8-0690-4d46-858c-1caa428edc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_batch = next(iter(datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0668e805-372e-4f1d-ba9e-093dff47ee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ratings'])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1b2ddcbe-c5c7-4bbc-9d40-923b445238bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['_user_index', 'userId', 'date', 'imdbId', 'titleType', 'genre', 'actor', 'director', 'runtimeMinutesCluster', 'startYearCluster'])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch['ratings'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2f1b4494-0046-42cc-8048-f106925508e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([5000, 2655], dtype=int32)>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch['ratings']['_user_index'].bounding_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a22a7-6fd5-4e51-8c8c-162afdebc689",
   "metadata": {},
   "source": [
    "For now we have less users in last batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7e5d4a30-5690-4029-a850-a434819cfc75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([3287, 1040], dtype=int32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for last_batch in datasets['train']:\n",
    "    pass\n",
    "last_batch['ratings']['_user_index'].bounding_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62718662-b0fc-4ccd-a5f3-c6119f08f764",
   "metadata": {},
   "source": [
    "Note that we kept local batch indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f1acf01-e2a7-4888-9301-d0c72d77a535",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[0, 1, 2, 3, 4],\n",
       " [12, 13, 14, 15, 16],\n",
       " [102, 103, 104, 105, 106]]>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_batch['ratings']['_user_index'][:3, :5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a3a30-d1ad-4a87-8cdc-2eadf89edb32",
   "metadata": {},
   "source": [
    "## Saving raw dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5c97e5e6-1cac-4420-ab64-7f367ac2777f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 35.2 s, sys: 300 ms, total: 35.5 s\n",
      "Wall time: 35.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for split_name, dataset in datasets.items():\n",
    "    tf.data.experimental.save(dataset, os.path.join(DATASETS_ROOT_DIR, f'movielens_imdb/raw_{split_name}_dataset.tf'),\n",
    "                              compression='GZIP')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1d2075-3427-4797-acaa-a490b3924109",
   "metadata": {},
   "source": [
    "## Aggregate preceding events"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d52837-f9e5-4568-8a9c-b60404789d73",
   "metadata": {},
   "source": [
    "Till now we have only features describing items. To describe users let's consider following features:\n",
    "* for each `userId`, `date` we look at events done on previous dates\n",
    "* independently for each item feature we construct lists of those features corresponding to preceding events\n",
    "\n",
    "So if a user have rated some films\n",
    "\n",
    "| **film** | **genre**   | **date** |\n",
    "|----------|-------------|----------|\n",
    "| `1`      | Comedy      | `20/01`  |\n",
    "| `9`      | Drama       | `25/01`  |\n",
    "| `8`      | Comedy      | `25/01`  |\n",
    "| `3`      | Documentary | `30/01`  |\n",
    "\n",
    "we will construct `aggregated_film` and `aggregated_genre` features as\n",
    "\n",
    "| **aggregated_film** | **aggregated_genre**    | **date** |\n",
    "|---------------------|-------------------------|----------|\n",
    "| []                  | []                      | `20/01`  |\n",
    "| [`1`]               | [Comedy]                | `25/01`  |\n",
    "| [`1`]               | [Comedy]                | `25/01`  |\n",
    "| [`1`, `9`, `8`]     | [Comedy, Drama, Comedy] | `30/01`   |\n",
    "\n",
    "and so for each user, for each date corresponding to events we want to predict (`ratings` here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "5f54e450-e49a-4935-981c-03db61c65294",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from utils import aggregate_preceding_events\n",
    "\n",
    "aggregated_datesets = {}\n",
    "for split_name, dataset in datasets.items():\n",
    "    agg_func = partial(aggregate_preceding_events, target='ratings', item_features=item_features,\n",
    "                       user_id_column='userId', date_column='date')\n",
    "    aggregated_datesets[split_name] = dataset.map(agg_func, num_parallel_calls=2, deterministic=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f3e55bf0-34ce-41e9-956b-f0a06f4f6752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['aggregated_ratings_imdbId', 'aggregated_ratings_titleType', 'aggregated_ratings_genre', 'aggregated_ratings_actor', 'aggregated_ratings_director', 'aggregated_ratings_runtimeMinutesCluster', 'aggregated_ratings_startYearCluster', 'imdbId', 'titleType', 'genre', 'actor', 'director', 'runtimeMinutesCluster', 'startYearCluster', 'userId', 'date'])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch = next(iter(aggregated_datesets['train']))\n",
    "first_batch.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7596b6-0770-4c69-ae89-59d9c524897a",
   "metadata": {},
   "source": [
    "So resulting structure contains\n",
    "* aggregated historical features: for each user (1st dim), each target event (2nd dim) we have a list of previous event's attributes (3rd dim)\n",
    "* raw item features, user id, date: for each user (1st dim) we have a list of target events (2nd dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0a71982-3cfc-4a73-a421-505b95e999ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5000, 2655,  100], dtype=int32)>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch['aggregated_ratings_genre'].bounding_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e6cc98d9-f8d2-40ba-8cfd-b05572fc0367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([5000, 2655], dtype=int32)>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_batch['genre'].bounding_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90b6f5dc-27f9-46cc-9d58-2713f79d14d3",
   "metadata": {},
   "source": [
    "## Saving resulting datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ad5c3cda-2b58-4057-a392-41b6e0898777",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 44s, sys: 6.09 s, total: 2min 50s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for split_name, dataset in aggregated_datesets.items():\n",
    "    tf.data.experimental.save(dataset, os.path.join(DATASETS_ROOT_DIR, f'movielens_imdb/aggregated_{split_name}_dataset.tf'),\n",
    "                              compression='GZIP')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [moksha-tf2-cpu.2-7] (Local)",
   "language": "python",
   "name": "local-eu.gcr.io_tinyclues-experiments_tinyclues_moksha-tf2-cpu.2-7_latest__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
