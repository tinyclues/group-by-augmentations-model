{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dfc222-2bde-4c04-8dc5-de119ffd597c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "domain = ''\n",
    "group_by_pipeline_uuid = ''\n",
    "two_tower_pipeline_uuid = ''\n",
    "model_name = ''\n",
    "\n",
    "from experimentations.pipeline_artifacts import PipelineArtifacts\n",
    "artifacts = PipelineArtifacts(domain, group_by_pipeline_uuid)\n",
    "\n",
    "datasets = {}\n",
    "datasets['train'], training_ds_for_eval, test_ds = artifacts.dataloaders(model_name)\n",
    "group_by_task, _ = artifacts.task_and_vectorization(model_name)\n",
    "vocabulary_sizes = group_by_task._model_kwargs['size_dict']\n",
    "offer_features = list(group_by_task.target_features)\n",
    "user_features = [feature for feature in vocabulary_sizes.keys() if feature not in offer_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82bd5b9-92be-4a46-b6df-3dafd7cf472d",
   "metadata": {},
   "outputs": [],
   "source": [
    "offer_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf7a805-1e00-4f12-8fb3-c61ee73fbd12",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = []\n",
    "assert TASKS, 'Choose some offer features for single task benchmarks'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7784472-0be2-400d-af3b-587384c34c14",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Preparing training and evaluation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893296c-5c45-400d-b41c-dfe193328186",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pop_response(batch):\n",
    "    y = batch.pop('response')\n",
    "    return batch, y\n",
    "\n",
    "datasets['test'] = test_ds['test'].map(pop_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1783b85b-1b55-4f6e-853a-4181149debd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from utils import prepare_single_task_dataset\n",
    "single_task_datasets = {}\n",
    "for task_offer_feature in TASKS:\n",
    "    single_task_datasets[task_offer_feature] = \\\n",
    "        prepare_single_task_dataset(datasets['test'], task_offer_feature, offer_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95113520-5f8d-4a34-8dd8-d6c5f91ffde3",
   "metadata": {},
   "source": [
    "## Training models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2c3ee-ed38-4266-b78c-f9298874f3ce",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Two tower model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f847c3f-7ea4-4eff-ad37-94a994e620b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from utils import WEIGHT_SUFFIX, BroadcastLoss, BroadcastMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407651dc-6661-4a6e-ad09-2ae1e56275af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One can take parameters from production config or change them\n",
    "group_by_task.model_kwargs, group_by_task._optimizer_kwargs, group_by_task.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f03959a-cc8f-4da8-9792-f2bd80860a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "EMBEDDING_DIM = 30\n",
    "L1_COEFF = 2e-7\n",
    "DROPOUT = 0.05\n",
    "\n",
    "\n",
    "def REGULARIZER():\n",
    "    return {'class_name': 'L1L2', 'config': {'l1': L1_COEFF, 'l2': 0.}}\n",
    "\n",
    "def USER_TOWER():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(30,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('gelu'),\n",
    "        tf.keras.layers.Dense(20,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('gelu'),\n",
    "    ], name='user_tower')\n",
    "\n",
    "def OFFER_TOWER():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(30,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('gelu'),\n",
    "        tf.keras.layers.Dense(20,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('gelu'),\n",
    "    ], name='offer_tower')\n",
    "\n",
    "EPOCHS = 3\n",
    "\n",
    "NUMBER_OF_NEGATIVES = 3\n",
    "LOSS = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "AUC_METRIC = tf.keras.metrics.AUC(from_logits=True)\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "OPTIMIZER = tfa.optimizers.AdamW(weight_decay=4e-7, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97005956-18db-4239-a4fd-79e97a750843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if these assertions are wrong, need to pass correct negative_positive_ratio in\n",
    "# BroadcastLoss, BroadcastMetric and further in evaluate_model\n",
    "assert NUMBER_OF_NEGATIVES == group_by_task.model_kwargs['negative_positive_ratio']\n",
    "assert NUMBER_OF_NEGATIVES == two_tower_task.model_kwargs['negative_positive_ratio']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe84864c-0ea3-47bb-9cfd-35144392dac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_two_tower_model(offer_features, name='two_tower_model'):\n",
    "    # user_features, vocabulary_sizes, EMBEDDING_DIM, REGULARIZER, USER_TOWER, OFFER_TOWER,\n",
    "    # OPTIMIZER, LOSS, NUMBER_OF_NEGATIVES\n",
    "    # come from global scope, but can be passed as params instead\n",
    "    embeddings, inputs = {}, {}\n",
    "    for feature in user_features + offer_features:\n",
    "        if feature in offer_features:\n",
    "            # for offer features we need weights:\n",
    "            # with dummy weights during training, and the ones used for a feature's averaging at inference time\n",
    "            inputs[f'{feature}{WEIGHT_SUFFIX}'] = get_input_layer(f'{feature}{WEIGHT_SUFFIX}', tf.float32)\n",
    "        inputs[feature] = get_input_layer(feature)\n",
    "        # here we use input feature modality from `vocabulary_sizes` to know embeddings matrix dimensions\n",
    "        emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                       EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                       embeddings_regularizer=REGULARIZER())\n",
    "        embeddings[feature] = emb_layer(inputs[feature], inputs.get(f'{feature}{WEIGHT_SUFFIX}'))\n",
    "    \n",
    "    embedded_user_features = [embeddings[feature] for feature in user_features]\n",
    "    embedded_offer_features = [embeddings[feature] for feature in offer_features]\n",
    "    user_tower = USER_TOWER()(tf.keras.layers.Concatenate(name='concat_user')(embedded_user_features))\n",
    "    offer_tower = OFFER_TOWER()(tf.keras.layers.Concatenate(name='concat_offer')(embedded_offer_features))\n",
    "    \n",
    "    output = DotWithNegatives(NUMBER_OF_NEGATIVES, name='prediction')([user_tower, offer_tower], generate_negatives=True)\n",
    "    model = tf.keras.Model(inputs, output, name=name)\n",
    "    model.compile(optimizer=OPTIMIZER,\n",
    "                  loss=BroadcastLoss(LOSS, NUMBER_OF_NEGATIVES),\n",
    "                  metrics=[BroadcastMetric(AUC_METRIC, NUMBER_OF_NEGATIVES)])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa62aae1-b726-4646-bf15-8d8218a99837",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_tower_model = get_two_tower_model(offer_features, name='two_tower_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d78867-2719-4b9a-85dd-a1e30cca3047",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_tower_model.fit(datasets['train'], epochs=EPOCHS, validation_data=datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3ce3a3-127c-48af-856d-97dc301032f2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Two tower like model defined with production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda1caab-e402-46fb-a94f-0a945fb9e44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "artifacts_two_tower = PipelineArtifacts(domain, wide_bi_pipeline_uuid)\n",
    "two_tower_task, _ = artifacts_two_tower.task_and_vectorization(model_name)\n",
    "\n",
    "# align vectorizations\n",
    "two_tower_task.training_data_schema_by_block = group_by_task.training_data_schema_by_block\n",
    "two_tower_task._model_kwargs['size_dict'] = group_by_task._model_kwargs['size_dict']\n",
    "two_tower_task._model = None\n",
    "\n",
    "two_tower_task.compile(two_tower_task.model)\n",
    "metrics, eval_metrics, _, _, _ = train_fn(two_tower_task, datasets['train'], training_ds_for_eval, test_ds,\n",
    "                                          verbose=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8c266e-0161-434b-b760-d3f473304c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd417c9d-85ba-43f1-a67f-6d5e554ef43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f38f65-73b4-42ef-bafa-dc3d5370d4de",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Group-by augmentations model from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e913941a-31af-40ec-89cd-f6db056cd98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_AUGMENTATIONS = 3\n",
    "AVERAGE_NUMBER_OF_FEATURES_IN_AUGMENTATION = 2\n",
    "USER_META_FEATURES = 7\n",
    "OFFER_META_FEATURES = 5\n",
    "\n",
    "def OUTPUT_DNN():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(30,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('gelu'),\n",
    "        tf.keras.layers.Dense(20,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('gelu'),\n",
    "        tf.keras.layers.Dense(1,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "    ], name='output_dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca9bb4-d465-4162-a852-40217328e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_group_by_augm_model(offer_features, name='group_by_model'):\n",
    "    # user_features, vocabulary_sizes, EMBEDDING_DIM, REGULARIZER, USER_TOWER, OFFER_TOWER,\n",
    "    # OPTIMIZER, LOSS, NUMBER_OF_NEGATIVES\n",
    "    # come from global scope, but can be passed as params instead\n",
    "    inputs = {}\n",
    "    embedded_user_features, embedded_offer_features, variance_offer_features = {}, {}, {}\n",
    "    for feature in user_features:\n",
    "        inputs[feature] = get_input_layer(feature)\n",
    "        emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                       EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                       embeddings_regularizer=REGULARIZER())\n",
    "        embedded_user_features[feature] = emb_layer(inputs[feature])\n",
    "    for feature in offer_features:\n",
    "        # for offer features we need weights:\n",
    "        # with dummy weights during training, and the ones used for a feature's averaging at inference time\n",
    "        inputs[f'{feature}_weight'] = get_input_layer(f'{feature}_weight', tf.float32)\n",
    "        inputs[feature] = get_input_layer(feature)\n",
    "        emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                       EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                       embeddings_regularizer=REGULARIZER(),\n",
    "                                       calculate_variance=True)\n",
    "        embedded_offer_features[feature], variance_offer_features[feature] =\\\n",
    "            emb_layer(inputs[feature], inputs[f'{feature}_weight'])\n",
    "    \n",
    "        \n",
    "    user_stacked = tf.stack(list(embedded_user_features.values()), axis=1)\n",
    "    offer_stacked = tf.stack(list(embedded_offer_features.values()), axis=1)\n",
    "    offer_variance = tf.stack(list(variance_offer_features.values()), axis=1)\n",
    "    stacked_raw_offer_attrs = tf.stack([tf.cast(inp.values, tf.int32) for feature, inp in inputs.items()\n",
    "                                        if feature in offer_features], axis=1)\n",
    "\n",
    "\n",
    "    group_by = GroupBy(name='group_by')\n",
    "    key_generator = KeyGenerator(number_of_offer_attributes=len(offer_features),\n",
    "                                 average_number_of_attributes_in_key=AVERAGE_NUMBER_OF_FEATURES_IN_AUGMENTATION,\n",
    "                                 name='grp_key_generator')\n",
    "\n",
    "    user_compressed = UserFeaturesCompressor(USER_META_FEATURES, DROPOUT,\n",
    "                                             name='user_compressor')(user_stacked)\n",
    "    offer_features_compressor = OfferFeaturesCompressor(OFFER_META_FEATURES, DROPOUT, name='offer_compressor')\n",
    "    mask_net = MaskNet(OFFER_META_FEATURES, DROPOUT, name='mask_generation')\n",
    "    apply_mask = tf.keras.layers.Multiply(name='apply_mask')\n",
    "    bi_linear_interaction = BiLinearInteraction(number_of_negatives=NUMBER_OF_NEGATIVES, dropout_rate=DROPOUT,\n",
    "                                                initializer='random_normal', regularizer=REGULARIZER(),\n",
    "                                                name='interaction')\n",
    "    output_dnn = OUTPUT_DNN()\n",
    "\n",
    "    augmentation_predictions = []\n",
    "    for i in range(NB_AUGMENTATIONS):\n",
    "        group_by_key = key_generator(stacked_raw_offer_attrs)\n",
    "        mean_offer_emb, variance_offer_emb = group_by(group_by_key, offer_stacked)\n",
    "        compressed_offer_embeddings = offer_features_compressor([mean_offer_emb, variance_offer_emb])\n",
    "        mask = mask_net([mean_offer_emb, variance_offer_emb])\n",
    "        masked_offer_embeddings = apply_mask([compressed_offer_embeddings, mask])\n",
    "        _output = output_dnn(bi_linear_interaction([user_compressed, masked_offer_embeddings], generate_negatives=True))\n",
    "        augmentation_predictions.append(_output)\n",
    "    output = tf.concat(augmentation_predictions, axis=1)\n",
    "    \n",
    "    compressed_offer_embeddings = offer_features_compressor([offer_stacked, offer_variance])\n",
    "    mask = mask_net([offer_stacked, offer_variance])\n",
    "    masked_offer_embeddings = apply_mask([compressed_offer_embeddings, mask])\n",
    "\n",
    "    eval_output = output_dnn(bi_linear_interaction([user_compressed, masked_offer_embeddings], generate_negatives=True))\n",
    "\n",
    "    model = tf.keras.Model(inputs, output, name=name)\n",
    "    model.compile(optimizer=OPTIMIZER,\n",
    "                  loss=BroadcastLoss(LOSS, NUMBER_OF_NEGATIVES),\n",
    "                  metrics=[BroadcastMetric(AUC_METRIC, NUMBER_OF_NEGATIVES)])\n",
    "\n",
    "    eval_model = tf.keras.Model(inputs, eval_output, name=f'{name}_eval')\n",
    "    \n",
    "    return model, eval_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a30dfb1-aba0-48e1-80ed-5db24f29f17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_model, group_by_eval_model = get_group_by_augm_model(offer_features, name='group_by_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2214dd02-68dc-4bb4-a67d-36ee2197aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_model.fit(datasets['train'], epochs=EPOCHS, validation_data=datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca76775-cb2c-486f-94dd-97c803783172",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Group-by augmentations model defined with production code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0c7a86-2dcd-4b95-a54f-eeeafe49722b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from moksha.helpers.trainer import train_fn\n",
    "metrics, eval_metrics, *_ = train_fn(group_by_task,\n",
    "                                     datasets['train'], training_ds_for_eval, test_ds,\n",
    "                                     verbose=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10490205-987e-4049-8b90-d5bc15ccc89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0eff8ba-e6dc-4dc6-b6e5-9bc297f34ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80467bf-f8e4-4e43-8b51-611b00a15ce5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Training baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4837b9-026c-416a-bc6f-ad9f9005aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bi_linear_interaction_model(single_task_feature, name='bi_linear_model'):\n",
    "    # user_features, vocabulary_sizes, EMBEDDING_DIM, REGULARIZER, USER_TOWER, OFFER_TOWER,\n",
    "    # OPTIMIZER, LOSS, NUMBER_OF_NEGATIVES\n",
    "    # come from global scope, but can be passed as params instead\n",
    "    inputs = {}\n",
    "    embedded_user_features, embedded_offer_features, variance_offer_features = {}, {}, {}\n",
    "    for feature in user_features:\n",
    "        inputs[feature] = get_input_layer(feature)\n",
    "        emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                       EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                       embeddings_regularizer=REGULARIZER())\n",
    "        embedded_user_features[feature] = emb_layer(inputs[feature])\n",
    "\n",
    "    # for offer feature we need weights:\n",
    "    # with dummy weights during training, and the ones used for a feature's averaging at inference time\n",
    "    inputs[f'{single_task_feature}_weight'] = get_input_layer(f'{single_task_feature}_weight', tf.float32)\n",
    "    inputs[single_task_feature] = get_input_layer(single_task_feature)\n",
    "    emb_layer = WeightedEmbeddings(vocabulary_sizes[single_task_feature],\n",
    "                                   EMBEDDING_DIM, name=f'{single_task_feature}_embedding',\n",
    "                                   embeddings_regularizer=REGULARIZER())\n",
    "    embedded_offer_feature = emb_layer(inputs[single_task_feature],\n",
    "                                       inputs[f'{single_task_feature}_weight'])\n",
    "    \n",
    "    user_stacked = tf.stack(list(embedded_user_features.values()), axis=1)\n",
    "    offer_stacked = tf.expand_dims(embedded_offer_feature, axis=1)\n",
    "    \n",
    "    \n",
    "    user_compressed = UserFeaturesCompressor(USER_META_FEATURES, DROPOUT,\n",
    "                                             name='user_compressor')(user_stacked)\n",
    "    mask_net = MaskNet(OFFER_META_FEATURES, DROPOUT, name='mask_generation')\n",
    "    apply_mask = tf.keras.layers.Multiply(name='apply_mask')\n",
    "    bi_linear_interaction = BiLinearInteraction(number_of_negatives=NUMBER_OF_NEGATIVES, dropout_rate=DROPOUT,\n",
    "                                                initializer='random_normal', regularizer=REGULARIZER(),\n",
    "                                                name='interaction')\n",
    "    output_dnn = OUTPUT_DNN()\n",
    "\n",
    "    \n",
    "    mask = mask_net([offer_stacked, offer_stacked])\n",
    "    masked_offer_embeddings = apply_mask([offer_stacked, mask])\n",
    "    \n",
    "    output = OUTPUT_DNN()(bi_linear_interaction([user_compressed, masked_offer_embeddings],\n",
    "                                                generate_negatives=True))\n",
    "\n",
    "    model = tf.keras.Model(inputs, output, name=name)\n",
    "    model.compile(optimizer=OPTIMIZER,\n",
    "                  loss=BroadcastLoss(LOSS, NUMBER_OF_NEGATIVES),\n",
    "                  metrics=[BroadcastMetric(AUC_METRIC, NUMBER_OF_NEGATIVES)])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77d5872-118d-4907-964b-84bbdd44d664",
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_feature_models = {}\n",
    "for task_offer_feature in TASKS:\n",
    "    mono_feature_models[task_offer_feature] =\\\n",
    "        bi_linear_interaction_model(task_offer_feature, name=f'{task_offer_feature}_model')\n",
    "    mono_feature_models[task_offer_feature].fit(datasets['train'],\n",
    "                                                epochs=EPOCHS,\n",
    "                                                validation_data=datasets['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "067ef8b8-4e4d-4403-bd64-449a2379eed4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Evaluation and comparison with baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9d324a-eb2e-43d6-8ee7-cd83e64e2915",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TO_EVAL = {\n",
    "    'two_tower': two_tower_model,\n",
    "    'group_by': group_by_eval_model,\n",
    "    'prod two_tower': two_tower_task.model,\n",
    "    'prod group_by': group_by_task.model,\n",
    "    **{f'Mono:{task_offer_feature}': mono_feature_models[task_offer_feature]\n",
    "       for task_offer_feature in TASKS}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bfa3f7-7e06-489c-b755-3df84ee4b0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "from utils import evaluate_model\n",
    "\n",
    "aucs = defaultdict(dict)\n",
    "\n",
    "for task_offer_feature in TASKS:\n",
    "    for model_name, model in MODEL_TO_EVAL.items():\n",
    "        print(task_offer_feature, model_name)\n",
    "        aucs[task_offer_feature][model_name] = evaluate_model(model, task_offer_feature,\n",
    "                                                              single_task_datasets, NUMBER_OF_NEGATIVES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93600e63-4ef6-423c-8c4a-47faf0e67cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(f'_reports/{domain}_eval.pickle', 'wb') as f:\n",
    "    pickle.dump(aucs, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19635898-af22-4af0-8c38-20c71f443d2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Reporting wAUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7eff9c-1b84-497f-bbea-68d0862998ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wAUC(auc_df, cutoff=200):\n",
    "    auc_df = auc_df[(auc_df.index != 0) & (auc_df['number of events'] > cutoff)]\n",
    "    return (auc_df['auc'] * auc_df['number of events']).sum() / auc_df['number of events'].sum()\n",
    "\n",
    "\n",
    "def meanAUC(auc_df, cutoff=200):\n",
    "    auc_df = auc_df[(auc_df.index != 0) & (auc_df['number of events'] > cutoff)]\n",
    "    return auc_df['auc'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73c36f2-4842-4ab9-acf9-f2dd411aae2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for task_name in aucs:\n",
    "    for model_name in aucs[task_name]:\n",
    "        w_auc = np.round(wAUC(aucs[task_name][model_name]), 3)\n",
    "        results = results.append({'wAUC': w_auc, 'offers': task_name, 'model': model_name}, ignore_index=True)\n",
    "\n",
    "pd.pivot_table(results, 'wAUC', 'model', 'offers').style.background_gradient(cmap='coolwarm')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [moksha-tf2-cpu.2-7] (Local)",
   "language": "python",
   "name": "local-eu.gcr.io_tinyclues-experiments_tinyclues_moksha-tf2-cpu.2-7_latest__python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
