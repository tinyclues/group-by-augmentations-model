{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea932e5b",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b6f05f",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d96897ae",
   "metadata": {},
   "source": [
    "Loading and preparing dataset as in [the training for Movielens/IMDB dataset](https://github.com/tinyclues/recsys-multi-atrribute-benchmark/blob/master/training/movielens%20simple%20model.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1899e738-56d3-455f-afcd-ce80fcb17232",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'rees_ecommerce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cc05cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_dataset\n",
    "\n",
    "datasets = {}\n",
    "for split_name in ['train', 'val', 'test']:\n",
    "    datasets[split_name] = load_dataset(DATASET, split_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5077ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import AGG_PREFIX\n",
    "\n",
    "all_columns = list(datasets['train'].element_spec.keys())\n",
    "technical_columns = ['user_id', 'date']\n",
    "user_features = list(filter(lambda x: x.startswith(AGG_PREFIX), all_columns))\n",
    "offer_features = list(filter(lambda x: x not in user_features + technical_columns, all_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa53a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from uuid import uuid4\n",
    "\n",
    "from utils import rebatch_by_events\n",
    "\n",
    "datasets['train'] = rebatch_by_events(datasets['train'], batch_size=5040, date_column='date', nb_events_by_user_by_day=8)\n",
    "for key in ['val', 'test']:\n",
    "    datasets[key] = rebatch_by_events(datasets[key], batch_size=5040, date_column='date', nb_events_by_user_by_day=8,\n",
    "                                      seed=1729).cache(f'/tmp/{uuid4()}.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20173463-08c7-4b6f-a9b8-ade7c5c1121f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_equal_weights\n",
    "\n",
    "for key in datasets:\n",
    "    datasets[key] = datasets[key].map(partial(add_equal_weights, features=offer_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb20fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_inverse_lookups\n",
    "inverse_lookups = load_inverse_lookups(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a92d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "vocabulary_sizes = {}\n",
    "\n",
    "for feature in offer_features:\n",
    "    vocabulary_sizes[feature] = inverse_lookups[feature].vocabulary_size()\n",
    "\n",
    "for feature in user_features:\n",
    "    for key in inverse_lookups:\n",
    "        pattern = re.compile(r\"{}(\\w+)_{}\".format(AGG_PREFIX, key))\n",
    "        if pattern.match(feature):\n",
    "            vocabulary_sizes[feature] = vocabulary_sizes[key]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f569ab-efb6-4307-8a27-98d64c62f863",
   "metadata": {},
   "source": [
    "## Prepare evaluation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60cc0793-ec4b-4107-b741-2f7a2220ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "TASKS = ['product_id', 'category1', 'category2', 'category3', 'brand', 'priceCluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6967d5c9-4f85-4e16-ba55-f3740c747d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from utils import get_task_offer_features, remap_features_using_key\n",
    "test_datasets, test_offer_tensors = {}\n",
    "for task_offer_feature in TASKS:\n",
    "    test_offer_tensors[task_offer_feature] = \\\n",
    "        get_task_offer_features(datasets['test'], task_offer_feature, offer_features)\n",
    "    test_datasets[task_offer_feature] = \\\n",
    "        remap_features_using_key(datasets['test'], task_offer_feature,\n",
    "                                 test_offer_tensors[task_offer_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6ac2fd-7ebe-4db2-bd48-4e9b6a2b2f60",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfa438-d348-4e5e-9c33-281f39cf6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51295282-14c2-4bca-828d-a9711a233fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "EMBEDDING_DIM = 100\n",
    "L1_COEFF = 3e-7\n",
    "DROPOUT = 0.1\n",
    "\n",
    "\n",
    "def REGULARIZER():\n",
    "    return {'class_name': 'L1L2', 'config': {'l1': L1_COEFF, 'l2': 0.}}\n",
    "\n",
    "def USER_TOWER():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('tanh'),\n",
    "        tf.keras.layers.Dense(50,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('tanh'),\n",
    "    ], name='user_tower')\n",
    "\n",
    "def OFFER_TOWER():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('tanh'),\n",
    "        tf.keras.layers.Dense(50,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('tanh'),\n",
    "    ], name='offer_tower')\n",
    "\n",
    "EPOCHS = 10\n",
    "\n",
    "NUMBER_OF_NEGATIVES = 4\n",
    "LOSS = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "AUC_METRIC = tf.keras.metrics.AUC(from_logits=True)\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "OPTIMIZER = tfa.optimizers.AdamW(weight_decay=4e-8, learning_rate=0.00085)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9becec0-faca-44ca-a8a8-23e1fdade0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_AUGMENTATIONS = 3\n",
    "AVERAGE_NUMBER_OF_FEATURES_IN_AUGMENTATION = 2\n",
    "USER_META_FEATURES = 5\n",
    "OFFER_META_FEATURES = 3\n",
    "\n",
    "\n",
    "def OUTPUT_DNN():\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(100,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('tanh'),\n",
    "        tf.keras.layers.Dense(50,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "        tf.keras.layers.Dropout(DROPOUT),\n",
    "        tf.keras.layers.Activation('tanh'),\n",
    "        tf.keras.layers.Dense(1,\n",
    "                              kernel_regularizer=REGULARIZER(),\n",
    "                              bias_regularizer=REGULARIZER()),\n",
    "    ], name='output_dnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd93a89-2741-4664-acf3-3ed406043787",
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers import *\n",
    "from utils import *\n",
    "\n",
    "def _model(group_by=False, mask_net=False, bilinear_interaction=False, name='model'):\n",
    "    inputs = {}\n",
    "    embedded_user_features, embedded_offer_features, variance_offer_features = {}, {}, {}\n",
    "    for feature in user_features:\n",
    "        inputs[feature] = get_input_layer(feature)\n",
    "        emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                       EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                       embeddings_regularizer=REGULARIZER())\n",
    "        embedded_user_features[feature] = emb_layer(inputs[feature])\n",
    "    for feature in offer_features:\n",
    "        # for offer features we need weights:\n",
    "        # with dummy weights during training, and the ones used for a feature's averaging at inference time\n",
    "        inputs[f'{feature}_weight'] = get_input_layer(f'{feature}_weight', tf.float32)\n",
    "        inputs[feature] = get_input_layer(feature)\n",
    "        emb_layer = WeightedEmbeddings(vocabulary_sizes[feature],\n",
    "                                       EMBEDDING_DIM, name=f'{feature}_embedding',\n",
    "                                       embeddings_regularizer=REGULARIZER(),\n",
    "                                       calculate_variance=True)\n",
    "        embedded_offer_features[feature], variance_offer_features[feature] =\\\n",
    "            emb_layer(inputs[feature], inputs[f'{feature}_weight'])\n",
    "    \n",
    "    user_stacked = tf.stack(list(embedded_user_features.values()), axis=1)\n",
    "    offer_stacked = tf.stack(list(embedded_offer_features.values()), axis=1)\n",
    "    offer_variance = tf.stack(list(variance_offer_features.values()), axis=1)\n",
    "    stacked_raw_offer_attrs = tf.stack([tf.cast(inp.values, tf.int32) for feature, inp in inputs.items()\n",
    "                                        if feature in offer_features], axis=1)\n",
    "    \n",
    "    if group_by:\n",
    "        group_by_layer = GroupBy(name='group_by')\n",
    "        key_generator = KeyGenerator(number_of_offer_attributes=len(offer_features),\n",
    "                                     average_number_of_attributes_in_key=AVERAGE_NUMBER_OF_FEATURES_IN_AUGMENTATION,\n",
    "                                     name='grp_key_generator')\n",
    "        \n",
    "        augmentations = []\n",
    "        for i in range(NB_AUGMENTATIONS):\n",
    "            group_by_key = key_generator(stacked_raw_offer_attrs)\n",
    "            augmentations.append(group_by_layer(group_by_key, offer_stacked))\n",
    "        \n",
    "    else:\n",
    "        augmentations = [(offer_stacked, None)]\n",
    "        \n",
    "    if mask_net:\n",
    "        user_compressed = UserFeaturesCompressor(USER_META_FEATURES, DROPOUT,\n",
    "                                                 name='user_compressor')(user_stacked)\n",
    "        \n",
    "        offer_features_compressor = OfferFeaturesCompressor(OFFER_META_FEATURES, DROPOUT, name='offer_compressor')\n",
    "        mask_net = MaskNet(OFFER_META_FEATURES, DROPOUT, name='mask_generation')\n",
    "        apply_mask = tf.keras.layers.Multiply(name='apply_mask')\n",
    "        \n",
    "        attention_augmentations = []\n",
    "        for mean_offer_emb, variance_offer_emb in augmentations:\n",
    "            compressed_offer_embeddings = offer_features_compressor([mean_offer_emb, variance_offer_emb])\n",
    "            mask = mask_net([mean_offer_emb, variance_offer_emb])\n",
    "            attention_augmentations.append(apply_mask([compressed_offer_embeddings, mask]))\n",
    "        \n",
    "        compressed_offer_embeddings = offer_features_compressor([offer_stacked, offer_variance])\n",
    "        mask = mask_net([offer_stacked, offer_variance])\n",
    "        eval_offer_embeddings = apply_mask([compressed_offer_embeddings, mask])\n",
    "    else:\n",
    "        user_compressed = user_stacked\n",
    "        attention_augmentations = [mean_offer_emb for mean_offer_emb, _ in augmentations]\n",
    "        eval_offer_embeddings = offer_stacked\n",
    "        \n",
    "    if bilinear_interaction:\n",
    "        if not mask_net:\n",
    "            # we need to apply compression to keep model's footprint limited\n",
    "            # and also to keep model robust with same hyperparams\n",
    "            user_compressed = UserFeaturesCompressor(USER_META_FEATURES, DROPOUT,\n",
    "                                                     name='user_compressor')(user_compressed)\n",
    "            \n",
    "        bi_linear_interaction = BiLinearInteraction(number_of_negatives=NUMBER_OF_NEGATIVES, dropout_rate=DROPOUT,\n",
    "                                                    initializer='random_normal', regularizer=REGULARIZER(),\n",
    "                                                    name='interaction')\n",
    "        output_dnn = OUTPUT_DNN()\n",
    "        \n",
    "        augmentation_predictions = []\n",
    "        for masked_offer_embeddings in attention_augmentations:\n",
    "            augmentation_predictions.append(\n",
    "                output_dnn(bi_linear_interaction([user_compressed, masked_offer_embeddings], generate_negatives=True))\n",
    "            )\n",
    "        output = tf.concat(augmentation_predictions, axis=1)\n",
    "        \n",
    "        eval_output = output_dnn(bi_linear_interaction([user_compressed, eval_offer_embeddings], generate_negatives=True))\n",
    "    else:\n",
    "        user_tower = USER_TOWER()(tf.keras.layers.Reshape((-1,), name='concat_user')(user_compressed))\n",
    "        \n",
    "        offer_reshape = tf.keras.layers.Reshape((-1,), name='concat_offer')\n",
    "        offer_tower_layer = OFFER_TOWER()\n",
    "        dot_interaction = DotWithNegatives(NUMBER_OF_NEGATIVES, name='prediction')\n",
    "        augmentation_predictions = []\n",
    "        for masked_offer_embeddings in attention_augmentations:\n",
    "            offer_tower = offer_tower_layer(offer_reshape(masked_offer_embeddings))\n",
    "            augmentation_predictions.append(dot_interaction([user_tower, offer_tower], generate_negatives=True))\n",
    "        output = tf.concat(augmentation_predictions, axis=1)\n",
    "        \n",
    "        eval_offer_embeddings = offer_tower_layer(offer_reshape(eval_offer_embeddings))\n",
    "        eval_output = dot_interaction([user_tower, eval_offer_embeddings],\n",
    "                                      generate_negatives=True)\n",
    "    \n",
    "    model = tf.keras.Model(inputs, output, name=name)\n",
    "    model.compile(optimizer=OPTIMIZER,\n",
    "                  loss=BroadcastLoss(LOSS, NUMBER_OF_NEGATIVES),\n",
    "                  metrics=[BroadcastMetric(AUC_METRIC, NUMBER_OF_NEGATIVES)])\n",
    "\n",
    "    eval_model = tf.keras.Model(inputs, eval_output, name=f'{name}_eval')\n",
    "    emb_model = tf.keras.Model(inputs, eval_offer_embeddings, name=f'{name}_emb')\n",
    "    \n",
    "    return model, eval_model, emb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9163cb-f025-496b-9e9c-f803d6096dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "MODELS, EVAL_MODELS, EMBEDDING_MODELS = {}, {}, {}\n",
    "for group_by, mask_net, bilinear_interaction in [(True, True, True), (False, False, True)]:\n",
    "    if mask_net and not group_by:\n",
    "        continue\n",
    "    key = group_by, mask_net, bilinear_interaction\n",
    "    MODELS[key], EVAL_MODELS[key], EMBEDDING_MODELS[key] = \\\n",
    "        _model(*key, name='_'.join(map(lambda x: str(x).lower(), key)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa9afb79-a295-4c19-b29c-d241f5a7928e",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9c9a0e-74cf-4537-90d1-cc9fedb2202a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(MODELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed435217-0d37-4c1d-87e0-9a1b05bbbd78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model in MODELS.values():\n",
    "    model.fit(datasets['train'], epochs=EPOCHS, validation_data=datasets['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043b45dc-b10d-4dfa-95aa-ba5b4cc40ce0",
   "metadata": {},
   "source": [
    "## Embeddings similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7703983a-47cd-43e9-9746-d21c609dd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_with_augmentations = EMBEDDING_MODELS[(True, True, True)]\n",
    "model_wo_augmentations = EMBEDDING_MODELS[(False, False, True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4084d451",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import AGG_PREFIX\n",
    "\n",
    "all_columns = list(datasets['train'].element_spec.keys())\n",
    "technical_columns = ['user_id', 'date']\n",
    "user_features = list(filter(lambda x: x.startswith(AGG_PREFIX), all_columns))\n",
    "offer_features = list(filter(lambda x: x not in user_features + technical_columns, all_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d40c729",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from uuid import uuid4\n",
    "\n",
    "from utils import rebatch_by_events\n",
    "\n",
    "datasets['train'] = rebatch_by_events(datasets['train'], batch_size=5040, date_column='date', nb_events_by_user_by_day=8)\n",
    "for key in ['val', 'test']:\n",
    "    datasets[key] = rebatch_by_events(datasets[key], batch_size=5040, date_column='date', nb_events_by_user_by_day=8,\n",
    "                                      seed=1729).cache(f'/tmp/{uuid4()}.tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1dbd7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_equal_weights\n",
    "\n",
    "for key in datasets:\n",
    "    datasets[key] = datasets[key].map(partial(add_equal_weights, features=offer_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import load_inverse_lookups\n",
    "inverse_lookups = load_inverse_lookups(DATASET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26dedec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "vocabulary_sizes = {}\n",
    "\n",
    "for feature in offer_features:\n",
    "    vocabulary_sizes[feature] = inverse_lookups[feature].vocabulary_size()\n",
    "\n",
    "for feature in user_features:\n",
    "    for key in inverse_lookups:\n",
    "        pattern = re.compile(r\"{}(\\w+)_{}\".format(AGG_PREFIX, key))\n",
    "        if pattern.match(feature):\n",
    "            vocabulary_sizes[feature] = vocabulary_sizes[key]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Custom [moksha-tf2-cpu.2-7] (Local)",
   "language": "python",
   "name": "local-eu.gcr.io_tinyclues-experiments_tinyclues_moksha-tf2-cpu.2-7_latest__moksha"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
